{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('screen_info.txt','rb') as fl:\n",
    "    t = pickle.load(fl)\n",
    "fnames = t[0]\n",
    "totf = t[1]\n",
    "binf = t[2]\n",
    "runfile = 0\n",
    "fname = fnames[runfile]\n",
    "bf = binf[runfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/bioassay-datasets/'\n",
    "p_fingerprints = []\n",
    "c_fingerprints = []\n",
    "labels = []\n",
    "with open(path+fname+'red_train.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints.append(row[:bf])\n",
    "        c_fingerprints.append(row[bf:-1])\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 112)\n",
      "('total no of 1s', 25982)\n",
      "('total no of 0s', 357394)\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "p_fingerprints = p_fingerprints.astype(int)\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples , ip_dim) = p_fingerprints.shape\n",
    "labels = labels[1:]\n",
    "print(no_examples,ip_dim)\n",
    "print(\"total no of 1s\",np.sum(p_fingerprints))\n",
    "print(\"total no of 0s\",no_examples*ip_dim-np.sum(p_fingerprints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fingerprints[(p_fingerprints==0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = np.zeros((len(labels),1))\n",
    "for i,l in enumerate(labels):\n",
    "    if l=='Active':\n",
    "        labels2[i] = 1\n",
    "    else:\n",
    "        labels2[i] = 0\n",
    "labels2 = labels2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\n"
     ]
    }
   ],
   "source": [
    "no_active_ele = (sum(labels2))\n",
    "print(no_active_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = ip_dim\n",
    "h1_dim = 500\n",
    "h2_dim = 500\n",
    "h3_dim = 500\n",
    "z_dim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size):\n",
    "    samples = np.random.randint(low=0,high=no_examples,size=(batch_size,1))\n",
    "    train_batch = p_fingerprints[samples].reshape(batch_size,ip_dim)\n",
    "    train_batch = train_batch.astype(int)\n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    target = Variable(torch.cuda.FloatTensor(labels2[samples]),requires_grad=False)\n",
    "    \n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(X_dim,h1_dim)\n",
    "        self.l2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h3_dim)\n",
    "        self.l4 = nn.Linear(h3_dim,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,h3_dim)\n",
    "        self.l2 = nn.Linear(h3_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h1_dim)\n",
    "        self.l4 = nn.Linear(h1_dim,X_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.tanh(self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim+2,500)\n",
    "        self.lin2 = nn.Linear(500,100)\n",
    "        self.lin3 = nn.Linear(100,100)\n",
    "        self.lin4 = nn.Linear(100,30)\n",
    "        self.lin5 = nn.Linear(30,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.lin1(x))\n",
    "        x = F.selu(self.lin2(x))\n",
    "        x = F.selu(self.lin3(x))\n",
    "        x = F.selu(self.lin4(x))\n",
    "        x = F.sigmoid(self.lin5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label_info(y,batch_size):\n",
    "\n",
    "    tmp = np.zeros((batch_size,2))\n",
    "    tmp2 = np.zeros((batch_size,1))\n",
    "    y = y.cpu().data.numpy().reshape(batch_size,1)\n",
    "    tmp2[y==0] = 5\n",
    "    tmp3 = np.zeros((batch_size,1))\n",
    "    tmp3[y==1] = 5\n",
    "    tmp = np.concatenate((tmp2,tmp3),1)\n",
    "    label_info = torch.from_numpy((tmp)).cuda()\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size):\n",
    "    \n",
    "    for it in range(1000):\n",
    "        x,y = get_train_batch(batch_size)\n",
    "        z = Q(x)\n",
    "\n",
    "        #Reconstruction\n",
    "        \n",
    "        x_recon = P(z)\n",
    "        '''\n",
    "        x_recon[x_recon<0] = 0\n",
    "        x_recon[x_recon>0] = 1\n",
    "        x_tar = Variable(torch.cuda.FloatTensor(x.size()),requires_grad=False)\n",
    "        x_tar[x==-1] = 0\n",
    "        x_tar[x==1] = 1'''\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        CEL = criterion(x_recon, x)\n",
    "        \n",
    "        CEL.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        P_solver.step()\n",
    "        \n",
    "        Q.zero_grad()\n",
    "        P.zero_grad()\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        label_info = (add_label_info(y,batch_size))\n",
    "        z_false = np.concatenate((z.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "        z_false = Variable(torch.FloatTensor(z_false)).cuda()\n",
    "        #z_false = torch.cat((z,label_info),1)\n",
    "        z_true = np.random.rand(batch_size,z_dim)\n",
    "        z_true = np.concatenate((z_true,label_info.cpu().numpy()),1)\n",
    "        z_true = Variable(torch.FloatTensor(z_true).cuda())\n",
    "        #z_true = torch.cat((z_true,label_info),1)\n",
    "        z_true_op = Variable(D(z_true).data,requires_grad=False)\n",
    "        \n",
    "        z_false_op = D(z_false)\n",
    "        add_small = 1e-20\n",
    "        \n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss_d = criterion(z_false_op,z_true_op)\n",
    "        #loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        D_solver.step()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(it%50==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', CEL.data[0],'disc_loss:', loss_d.data[0],'gen_loss: ',G_loss.data[0])\n",
    "            #print(x_recon[0][:50].cpu().data.numpy().T)\n",
    "            #print()\n",
    "            #print(x[0][:50].cpu().data.numpy().T)\n",
    "           # print()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    Q = encoder()\n",
    "    Q.cuda()\n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-4)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-5)\n",
    "    P = decoder()\n",
    "    P.cuda()\n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-4)\n",
    "    D = disc()\n",
    "    D.cuda()\n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-3)\n",
    "    batch_size = 120\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size)\n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 0.9969078898429871, 'disc_loss:', 0.6932688355445862, 'gen_loss: ', 0.6891515254974365)\n",
      "('recon_loss:', 0.20750296115875244, 'disc_loss:', 0.008507405407726765, 'gen_loss: ', 0.0005984064191579819)\n",
      "('recon_loss:', 0.2005728781223297, 'disc_loss:', 0.009099897928535938, 'gen_loss: ', 0.0005475477082654834)\n",
      "('recon_loss:', 0.1826290488243103, 'disc_loss:', 0.006507040932774544, 'gen_loss: ', 0.00046190060675144196)\n",
      "('recon_loss:', 0.16325703263282776, 'disc_loss:', 0.005109590478241444, 'gen_loss: ', 0.0005157768609933555)\n",
      "('recon_loss:', 0.1461239904165268, 'disc_loss:', 0.008810341358184814, 'gen_loss: ', 6.387197936419398e-05)\n",
      "('recon_loss:', 0.1343793272972107, 'disc_loss:', 0.012820517644286156, 'gen_loss: ', 0.0021916094701737165)\n",
      "('recon_loss:', 0.13820073008537292, 'disc_loss:', 0.03700462728738785, 'gen_loss: ', 0.003790489863604307)\n",
      "('recon_loss:', 0.12435507774353027, 'disc_loss:', 0.17500002682209015, 'gen_loss: ', 0.01797291450202465)\n",
      "('recon_loss:', 0.10237933695316315, 'disc_loss:', 0.09691172093153, 'gen_loss: ', 0.029929492622613907)\n",
      "('recon_loss:', 0.1039046123623848, 'disc_loss:', 0.023035015910863876, 'gen_loss: ', 0.011231761425733566)\n",
      "('recon_loss:', 0.09784183651208878, 'disc_loss:', 0.004147787112742662, 'gen_loss: ', 0.00023940671235322952)\n",
      "('recon_loss:', 0.08691383898258209, 'disc_loss:', 0.021110503003001213, 'gen_loss: ', 0.017453165724873543)\n",
      "('recon_loss:', 0.07494538277387619, 'disc_loss:', 0.00324235949665308, 'gen_loss: ', 0.00019354098185431212)\n",
      "('recon_loss:', 0.08258441835641861, 'disc_loss:', 0.02058437280356884, 'gen_loss: ', 0.01712898351252079)\n",
      "('recon_loss:', 0.07629461586475372, 'disc_loss:', 0.008298437111079693, 'gen_loss: ', 0.006131449714303017)\n",
      "('recon_loss:', 0.07589253783226013, 'disc_loss:', 0.013189650140702724, 'gen_loss: ', 0.01260652020573616)\n",
      "('recon_loss:', 0.07430373132228851, 'disc_loss:', 0.013758608140051365, 'gen_loss: ', 0.011717060580849648)\n",
      "('recon_loss:', 0.06284850090742111, 'disc_loss:', 0.013357283547520638, 'gen_loss: ', 0.011827290058135986)\n",
      "('recon_loss:', 0.06805127114057541, 'disc_loss:', 0.007301216013729572, 'gen_loss: ', 0.006264092866331339)\n"
     ]
    }
   ],
   "source": [
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_path = os.getcwd() + '/model_enc_' + str(fname)\n",
    "torch.save(Q.state_dict(),encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24234008789\n"
     ]
    }
   ],
   "source": [
    "#entire_batch,batch_labels = get_train_batch(no_examples)\n",
    "tic = time.time()\n",
    "## It takes too much memory. Split in chunks and \n",
    "z_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "toc = time.time()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_new_z = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate new samples from orignal ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if generate_new_z == True:\n",
    "    n_comb = 5\n",
    "    n_samples = 100\n",
    "    extra_samples = torch.cuda.FloatTensor(n_samples,z_dim)\n",
    "    extra_labels = Variable(torch.ones(n_samples).cuda())\n",
    "    for i in range(n_samples):\n",
    "        #coeff = np.random.rand(n_comb,1)\n",
    "        coeff = Variable(torch.randn(n_comb,1)).cuda()\n",
    "        active_z_encoded = z_encoded[torch.cuda.FloatTensor(labels2)==1]\n",
    "        tmp_rand_nos = torch.randperm(int(no_active_ele))\n",
    "        rand_nos = tmp_rand_nos[0:n_comb].cuda()\n",
    "        rand_z = torch.transpose(z_encoded[rand_nos],0,1)\n",
    "        extra_samples[i] = torch.cuda.FloatTensor(torch.matmul(rand_z,coeff).data)\n",
    "    extra_samples = Variable(extra_samples)\n",
    "    \n",
    "    new_z_encoded = torch.cat((z_encoded,extra_samples),0)\n",
    "    new_labels = torch.cat((Variable(torch.cuda.FloatTensor(labels2)),extra_labels),0)\n",
    "    perm = torch.randperm(no_examples+n_samples).cuda()\n",
    "    new_z_encoded = new_z_encoded[perm]\n",
    "    new_labels = new_labels[perm]\n",
    "    batch_labels_np = new_labels.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_encoded = z_encoded.cpu().data.numpy()[:,0]\n",
    "# y_encoded = z_encoded.cpu().data.numpy()[:,1]\n",
    "# w_encoded = z_encoded.cpu().data.numpy()[:,2]\n",
    "\n",
    "# # batch_labels_np = batch_labels_np.astype(int)\n",
    "# # print(batch_labels_np.dtype)\n",
    "# # print(batch_labels_np.shape)\n",
    "# batch_labels_np = list(labels2)\n",
    "\n",
    "# colors = []\n",
    "# for l in batch_labels_np:\n",
    "#     colors.append(\"C\"+str(int(l)))\n",
    "    \n",
    "# #plt.scatter(x_encoded,y_encoded,c=colors)\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(x_encoded,y_encoded,w_encoded,c=colors)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(size):\n",
    "    if generate_new_z == True:\n",
    "        print(\"gng here\")\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples+n_samples)[:size].numpy())\n",
    "        return new_z_encoded[ind], new_labels[ind]\n",
    "    else:\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples)[:size].numpy())\n",
    "        return z_encoded[ind], Variable(torch.cuda.LongTensor(labels2)[ind],requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,200)\n",
    "        self.l2 = nn.Linear(200,200)\n",
    "        self.l3 = nn.Linear(200,100)\n",
    "        self.l4 = nn.Linear(100,70)\n",
    "        self.l5 = nn.Linear(70,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.selu(self.l2(x))\n",
    "        x = F.selu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = (self.l5(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_disc_model(w):\n",
    "    d = Discriminator().cuda()\n",
    "    d_optim = optim.Adam(d.parameters(),lr=1e-4,weight_decay=1e-2)\n",
    "    d = train_disc(d,d_optim,w)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_disc(d,d_optim,w):\n",
    "    for ep in range(500):\n",
    "        d_optim.zero_grad()\n",
    "        x,true_l = sample_z(250)\n",
    "        true_l = true_l.view(true_l.size()[0],)\n",
    "        p_labels = d(x)\n",
    "        weights = torch.Tensor([1,w]).cuda()\n",
    "        criteria = nn.CrossEntropyLoss(weight=weights)\n",
    "        true_l = true_l.type(torch.cuda.LongTensor)\n",
    "        loss = criteria(p_labels,true_l)\n",
    "        loss.backward(retain_graph=True)\n",
    "        d_optim.step()\n",
    "        \n",
    "#         if(ep%50==49):\n",
    "#             print(loss.data[0])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.linspace(20,40,10)\n",
    "# with open(\"cnt_test_good)weights.txt\",'rb') as f:\n",
    "#     weights = pickle.load(f)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19849991798\n"
     ]
    }
   ],
   "source": [
    "# if generate_new_z == True:\n",
    "#     train_encoded = (new_z_encoded)\n",
    "#     labels_final = batch_labels_np\n",
    "# else:\n",
    "import time\n",
    "tic = time.time()\n",
    "train_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "toc = time.time()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_final = labels2\n",
    "fn_min  = 48\n",
    "    \n",
    "cm_autoencoder = []\n",
    "cm_autoencoder.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w: ', 20.0)\n",
      "197.015176058\n",
      "('tn, fp, fn, tp: ', array([3323,   52,   36,   12]))\n",
      "('saving model on weight: ', 20.0)\n",
      "('w: ', 22.222222222222221)\n",
      "82.819480896\n",
      "('tn, fp, fn, tp: ', array([3254,  121,   32,   16]))\n",
      "('saving model on weight: ', 22.222222222222221)\n",
      "('w: ', 24.444444444444443)\n",
      "32.1474499702\n",
      "('tn, fp, fn, tp: ', array([3152,  223,   27,   21]))\n",
      "('saving model on weight: ', 24.444444444444443)\n",
      "('w: ', 26.666666666666668)\n",
      "31.2467799187\n",
      "('tn, fp, fn, tp: ', array([3295,   80,   35,   13]))\n",
      "('w: ', 28.888888888888889)\n",
      "29.0178980827\n",
      "('tn, fp, fn, tp: ', array([3148,  227,   28,   20]))\n",
      "('w: ', 31.111111111111111)\n",
      "35.6970050335\n",
      "('tn, fp, fn, tp: ', array([3182,  193,   28,   20]))\n",
      "('w: ', 33.333333333333336)\n",
      "25.6418590546\n",
      "('tn, fp, fn, tp: ', array([3270,  105,   33,   15]))\n",
      "('w: ', 35.555555555555557)\n",
      "22.4232609272\n",
      "('tn, fp, fn, tp: ', array([3163,  212,   23,   25]))\n",
      "('saving model on weight: ', 35.555555555555557)\n",
      "('w: ', 37.777777777777779)\n",
      "20.9728498459\n",
      "('tn, fp, fn, tp: ', array([3327,   48,   38,   10]))\n",
      "('w: ', 40.0)\n",
      "39.8332881927\n",
      "('tn, fp, fn, tp: ', array([3101,  274,   26,   22]))\n"
     ]
    }
   ],
   "source": [
    "model_path = os.getcwd() + '/model_autoencoder_' + str(fname)\n",
    "for w in weights:\n",
    "    print(\"w: \",w)\n",
    "    tic = time.time()\n",
    "    d = gen_disc_model(w)\n",
    "    toc = time.time()\n",
    "    print(toc-tic)\n",
    "    ### Split the data while testing\n",
    "    train_op = np.zeros((no_examples,2))\n",
    "    tmpsize = int(np.floor(no_examples/5))\n",
    "    for j in range(5):\n",
    "        tmpop = d(train_encoded[(tmpsize*j):tmpsize*(j+1),:]).cpu().data.numpy()\n",
    "        train_op[(tmpsize*j):tmpsize*(j+1),:] = tmpop\n",
    "    train_op = np.argmax(train_op,axis=1)\n",
    "    cf = metrics.confusion_matrix(labels_final,train_op)\n",
    "    [tn, fp, fn, tp]  = cf.ravel()\n",
    "    print('tn, fp, fn, tp: ',cf.ravel())\n",
    "    if(fn < fn_min):\n",
    "        fn_min = fn\n",
    "        torch.save(d.state_dict(),model_path)\n",
    "        print(\"saving model on weight: \",w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 20.        ,  22.22222222,  24.44444444,  26.66666667,\n",
       "         28.88888889,  31.11111111,  33.33333333,  35.55555556,\n",
       "         37.77777778,  40.        ])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 20.        ,  22.22222222,  24.44444444,  26.66666667,\n",
       "         28.88888889,  31.11111111,  33.33333333,  35.55555556,\n",
       "         37.77777778,  40.        ])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"autoencoder_1.txt\",'wb') as fb:\n",
    "    pickle.dump(cm_autoencoder,fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
