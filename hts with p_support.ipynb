{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.metrics as metrics\n",
    "import numpy.linalg as la\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/daiict/CVShare/Jeni/hts/bioassay-datasets/'\n",
    "p_fingerprints = []\n",
    "labels = []\n",
    "with open(path+'AID362red_train.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints.append(row[:112])\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 112)\n",
      "('total no of 1s', 25982)\n",
      "('total no of 0s', 357394)\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "p_fingerprints = p_fingerprints.astype(int)\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples , ip_dim) = p_fingerprints.shape\n",
    "labels = labels[1:]\n",
    "print(no_examples,ip_dim)\n",
    "print(\"total no of 1s\",np.sum(p_fingerprints))\n",
    "print(\"total no of 0s\",no_examples*ip_dim-np.sum(p_fingerprints))\n",
    "\n",
    "p_fingerprints[(p_fingerprints==0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 48.]\n"
     ]
    }
   ],
   "source": [
    "labels2 = np.zeros((len(labels),1))\n",
    "for i,l in enumerate(labels):\n",
    "    if l=='Active':\n",
    "        labels2[i] = 1\n",
    "    else:\n",
    "        labels2[i] = 0\n",
    "\n",
    "no_active_ele = (sum(labels2))\n",
    "labels2 = labels2.astype(int)\n",
    "print(no_active_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size,keep_numpy=False):\n",
    "    samples = np.random.randint(low=0,high=no_examples,size=(batch_size,1))\n",
    "    train_batch = p_fingerprints[samples].reshape(batch_size,ip_dim)\n",
    "    train_batch = train_batch.astype(int)\n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    if(keep_numpy==False):\n",
    "        target = (torch.cuda.LongTensor(labels2[samples]))\n",
    "    else:\n",
    "        target = labels2[samples]\n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating p_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_active = np.float32(np.sum(p_fingerprints[labels2[:,0]==1],axis=0))/np.sum(labels2)\n",
    "p_inactive = np.float32(np.sum(p_fingerprints[labels2[:,0]==0],axis=0))/(no_examples-np.sum(labels2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying p_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_active = p_active + 0.5*(p_active-p_inactive)\n",
    "p_active[p_active<0] = 0\n",
    "p_inactive = p_inactive + 0.5*(p_inactive - p_active)\n",
    "p_inactive[p_inactive<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967415\n"
     ]
    }
   ],
   "source": [
    "#find the angle between them\n",
    "theta = np.dot(p_active,p_inactive)/(la.norm(p_active)*la.norm(p_inactive))\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(ip_dim,ip_dim+50)\n",
    "        self.l2 = nn.Linear(ip_dim+50,ip_dim)\n",
    "        self.l3 = nn.Linear(ip_dim,ip_dim)\n",
    "        self.l4 = nn.Linear(ip_dim,ip_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.l1(x))\n",
    "        x = F.tanh(self.l2(x))\n",
    "        x = F.tanh(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        #x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(ip_dim,ip_dim)\n",
    "        self.l2 = nn.Linear(ip_dim,ip_dim)\n",
    "        self.l3 = nn.Linear(ip_dim,ip_dim+50)\n",
    "        self.l4 = nn.Linear(ip_dim+50,ip_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.l1(x))\n",
    "        x = F.tanh(self.l2(x))\n",
    "        x = F.tanh(self.l3(x))\n",
    "        x = F.tanh(self.l4(x))\n",
    "        #x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.lin1 = nn.Linear(ip_dim,500)\n",
    "        self.lin2 = nn.Linear(500,500)\n",
    "        self.lin3 = nn.Linear(500,100)\n",
    "        self.lin4 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.lin1(x))\n",
    "        x = F.selu(self.lin2(x))\n",
    "        x = F.selu(self.lin3(x))\n",
    "        x = F.sigmoid(self.lin4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw samples based on labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_samples(batch_size,labels,var_active,var_inactive):\n",
    "    z = np.zeros((batch_size,ip_dim))\n",
    "    rand_vec = np.random.randn(batch_size,ip_dim)\n",
    "    z[labels==1] = p_active + var_active*rand_vec[labels==1]\n",
    "    z[labels==0] = p_inactive + var_inactive*rand_vec[labels==0]\n",
    "    z = torch.cuda.FloatTensor(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size):\n",
    "    \n",
    "    for it in range(4000):\n",
    "        x,y = get_train_batch(batch_size,keep_numpy=True)\n",
    "        y = y.reshape(batch_size,)\n",
    "        z = Q(x)\n",
    "        z_false = Q(x)\n",
    "        \n",
    "        \n",
    "        #Reconstruction\n",
    "        \n",
    "        x_recon = P(z)\n",
    "        criterion = nn.MSELoss()\n",
    "        CEL = criterion(x_recon, x)\n",
    "        CEL.backward()\n",
    "        \n",
    "        Q_solver.step()\n",
    "        P_solver.step()\n",
    "        \n",
    "        Q.zero_grad()\n",
    "        P.zero_grad()\n",
    "        \n",
    "        #Discriminator\n",
    "        z_false_op = D(z_false)\n",
    "        z_true = Variable(draw_samples(batch_size,y,var_active=0.5,var_inactive=0.2))\n",
    "        \n",
    "        \n",
    "        z_true_op = Variable(D(z_true).data,requires_grad=False)\n",
    "                \n",
    "        add_small = 1e-20\n",
    "        \n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss_d = criterion(z_false_op,z_true_op)\n",
    "        #loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "        loss_d.backward(retain_variables = True)\n",
    "        D_solver.step()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward()\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(it%50==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', CEL.data[0],'disc_loss:', loss_d.data[0],'gen_loss: ',G_loss.data[0])\n",
    "            #print(x_recon[0][:50].cpu().data.numpy().T)\n",
    "            #print()\n",
    "            #print(x[0][:50].cpu().data.numpy().T)\n",
    "           # print()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    Q = encoder()\n",
    "    Q.cuda()\n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-4)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-5)\n",
    "    P = decoder()\n",
    "    P.cuda()\n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-4)\n",
    "    D = disc()\n",
    "    D.cuda()\n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-3)\n",
    "    batch_size = 120\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size)\n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.py:92: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 1.0149848461151123, 'disc_loss:', 0.6930192112922668, 'gen_loss: ', 0.6953672766685486)\n",
      "('recon_loss:', 0.5983426570892334, 'disc_loss:', 0.47119948267936707, 'gen_loss: ', 0.03320678696036339)\n",
      "('recon_loss:', 0.2314487248659134, 'disc_loss:', 0.7718789577484131, 'gen_loss: ', 0.4064734876155853)\n",
      "('recon_loss:', 0.20331422984600067, 'disc_loss:', 0.8010252714157104, 'gen_loss: ', 0.3207401633262634)\n",
      "('recon_loss:', 0.2064284384250641, 'disc_loss:', 0.8395006656646729, 'gen_loss: ', 0.3377400040626526)\n",
      "('recon_loss:', 0.20806081593036652, 'disc_loss:', 0.8590247631072998, 'gen_loss: ', 0.3256811797618866)\n",
      "('recon_loss:', 0.20704840123653412, 'disc_loss:', 0.8567511439323425, 'gen_loss: ', 0.32617220282554626)\n",
      "('recon_loss:', 0.20011165738105774, 'disc_loss:', 0.8617870807647705, 'gen_loss: ', 0.3239302635192871)\n",
      "('recon_loss:', 0.1979791522026062, 'disc_loss:', 0.8534374237060547, 'gen_loss: ', 0.32579800486564636)\n",
      "('recon_loss:', 0.20327907800674438, 'disc_loss:', 0.8555273413658142, 'gen_loss: ', 0.32436469197273254)\n",
      "('recon_loss:', 0.1952967345714569, 'disc_loss:', 0.856113851070404, 'gen_loss: ', 0.32163140177726746)\n",
      "('recon_loss:', 0.20564037561416626, 'disc_loss:', 0.8551055788993835, 'gen_loss: ', 0.32391470670700073)\n",
      "('recon_loss:', 0.20396621525287628, 'disc_loss:', 0.8624381422996521, 'gen_loss: ', 0.3298925757408142)\n",
      "('recon_loss:', 0.1820964217185974, 'disc_loss:', 0.861811637878418, 'gen_loss: ', 0.3255510628223419)\n",
      "('recon_loss:', 0.18153278529644012, 'disc_loss:', 0.8676115870475769, 'gen_loss: ', 0.3276752233505249)\n",
      "('recon_loss:', 0.1698453575372696, 'disc_loss:', 0.8575490117073059, 'gen_loss: ', 0.3290821611881256)\n",
      "('recon_loss:', 0.16648639738559723, 'disc_loss:', 0.8473766446113586, 'gen_loss: ', 0.3217633366584778)\n",
      "('recon_loss:', 0.17512233555316925, 'disc_loss:', 0.8415457606315613, 'gen_loss: ', 0.312788188457489)\n",
      "('recon_loss:', 0.17495141923427582, 'disc_loss:', 0.8505146503448486, 'gen_loss: ', 0.3282569944858551)\n",
      "('recon_loss:', 0.17341765761375427, 'disc_loss:', 0.8468197584152222, 'gen_loss: ', 0.33321067690849304)\n",
      "('recon_loss:', 0.1627613604068756, 'disc_loss:', 0.855545699596405, 'gen_loss: ', 0.33027034997940063)\n",
      "('recon_loss:', 0.14492464065551758, 'disc_loss:', 0.862371563911438, 'gen_loss: ', 0.3242993652820587)\n",
      "('recon_loss:', 0.15099233388900757, 'disc_loss:', 0.8586124181747437, 'gen_loss: ', 0.32760462164878845)\n",
      "('recon_loss:', 0.15511296689510345, 'disc_loss:', 0.8498972058296204, 'gen_loss: ', 0.32943862676620483)\n",
      "('recon_loss:', 0.15226776897907257, 'disc_loss:', 0.836983859539032, 'gen_loss: ', 0.3017750084400177)\n",
      "('recon_loss:', 0.1416606307029724, 'disc_loss:', 0.8624789118766785, 'gen_loss: ', 0.3128477931022644)\n",
      "('recon_loss:', 0.15540899336338043, 'disc_loss:', 0.8473589420318604, 'gen_loss: ', 0.35218626260757446)\n",
      "('recon_loss:', 0.15309129655361176, 'disc_loss:', 0.8521329164505005, 'gen_loss: ', 0.29338106513023376)\n",
      "('recon_loss:', 0.14158542454242706, 'disc_loss:', 0.8612107038497925, 'gen_loss: ', 0.37662336230278015)\n",
      "('recon_loss:', 0.15236525237560272, 'disc_loss:', 0.7701143622398376, 'gen_loss: ', 0.2630830705165863)\n",
      "('recon_loss:', 0.14234618842601776, 'disc_loss:', 0.8266317844390869, 'gen_loss: ', 0.2974504828453064)\n",
      "('recon_loss:', 0.14784389734268188, 'disc_loss:', 0.8521066308021545, 'gen_loss: ', 0.26694005727767944)\n",
      "('recon_loss:', 0.12291451543569565, 'disc_loss:', 0.8306676149368286, 'gen_loss: ', 0.2826782464981079)\n",
      "('recon_loss:', 0.14709888398647308, 'disc_loss:', 0.8290243148803711, 'gen_loss: ', 0.2789854407310486)\n",
      "('recon_loss:', 0.135023295879364, 'disc_loss:', 0.8187172412872314, 'gen_loss: ', 0.28617507219314575)\n",
      "('recon_loss:', 0.14105668663978577, 'disc_loss:', 0.8382869958877563, 'gen_loss: ', 0.28011876344680786)\n",
      "('recon_loss:', 0.12532669305801392, 'disc_loss:', 0.8352453708648682, 'gen_loss: ', 0.27072980999946594)\n",
      "('recon_loss:', 0.1316244900226593, 'disc_loss:', 0.8354734182357788, 'gen_loss: ', 0.2759109139442444)\n",
      "('recon_loss:', 0.12271499633789062, 'disc_loss:', 0.8270469307899475, 'gen_loss: ', 0.2784700095653534)\n",
      "('recon_loss:', 0.13321934640407562, 'disc_loss:', 0.8156049251556396, 'gen_loss: ', 0.28427451848983765)\n",
      "('recon_loss:', 0.13257695734500885, 'disc_loss:', 0.8071351647377014, 'gen_loss: ', 0.2858908772468567)\n",
      "('recon_loss:', 0.12325406074523926, 'disc_loss:', 0.8229196071624756, 'gen_loss: ', 0.27691030502319336)\n",
      "('recon_loss:', 0.11729060858488083, 'disc_loss:', 0.8305937051773071, 'gen_loss: ', 0.27777281403541565)\n",
      "('recon_loss:', 0.1233171746134758, 'disc_loss:', 0.806864857673645, 'gen_loss: ', 0.2732527554035187)\n",
      "('recon_loss:', 0.11713579297065735, 'disc_loss:', 0.8204279541969299, 'gen_loss: ', 0.262927383184433)\n",
      "('recon_loss:', 0.13088837265968323, 'disc_loss:', 0.8288482427597046, 'gen_loss: ', 0.2652919590473175)\n",
      "('recon_loss:', 0.10911841690540314, 'disc_loss:', 0.8014736175537109, 'gen_loss: ', 0.27803662419319153)\n",
      "('recon_loss:', 0.13061736524105072, 'disc_loss:', 0.8551607728004456, 'gen_loss: ', 0.27005115151405334)\n",
      "('recon_loss:', 0.11683399230241776, 'disc_loss:', 0.8371192812919617, 'gen_loss: ', 0.2678406536579132)\n",
      "('recon_loss:', 0.12518860399723053, 'disc_loss:', 0.8254709243774414, 'gen_loss: ', 0.2929651141166687)\n",
      "('recon_loss:', 0.10769225656986237, 'disc_loss:', 0.8413487672805786, 'gen_loss: ', 0.2764933705329895)\n",
      "('recon_loss:', 0.11555622518062592, 'disc_loss:', 0.8258196711540222, 'gen_loss: ', 0.2923755645751953)\n",
      "('recon_loss:', 0.11240514367818832, 'disc_loss:', 0.8447259068489075, 'gen_loss: ', 0.2861456871032715)\n",
      "('recon_loss:', 0.11455800384283066, 'disc_loss:', 0.8317998051643372, 'gen_loss: ', 0.2849922776222229)\n",
      "('recon_loss:', 0.11766122281551361, 'disc_loss:', 0.8154019713401794, 'gen_loss: ', 0.2829778492450714)\n",
      "('recon_loss:', 0.10675254464149475, 'disc_loss:', 0.8337017297744751, 'gen_loss: ', 0.2585394084453583)\n",
      "('recon_loss:', 0.10466670989990234, 'disc_loss:', 0.8295354247093201, 'gen_loss: ', 0.2663593590259552)\n",
      "('recon_loss:', 0.11283227801322937, 'disc_loss:', 0.8283828496932983, 'gen_loss: ', 0.23110002279281616)\n",
      "('recon_loss:', 0.10734272748231888, 'disc_loss:', 0.7878080010414124, 'gen_loss: ', 0.21599188446998596)\n",
      "('recon_loss:', 0.10411284863948822, 'disc_loss:', 0.728259265422821, 'gen_loss: ', 0.25799620151519775)\n",
      "('recon_loss:', 0.10870780050754547, 'disc_loss:', 0.7743330597877502, 'gen_loss: ', 0.21490351855754852)\n",
      "('recon_loss:', 0.11322610825300217, 'disc_loss:', 0.8191009759902954, 'gen_loss: ', 0.20581072568893433)\n",
      "('recon_loss:', 0.1008208766579628, 'disc_loss:', 0.8339478969573975, 'gen_loss: ', 0.21511133015155792)\n",
      "('recon_loss:', 0.11880689114332199, 'disc_loss:', 0.7236565947532654, 'gen_loss: ', 0.23484429717063904)\n",
      "('recon_loss:', 0.11109039932489395, 'disc_loss:', 0.7662405371665955, 'gen_loss: ', 0.21981053054332733)\n",
      "('recon_loss:', 0.124958336353302, 'disc_loss:', 0.798818826675415, 'gen_loss: ', 0.21504218876361847)\n",
      "('recon_loss:', 0.1032043993473053, 'disc_loss:', 0.7556111216545105, 'gen_loss: ', 0.21853385865688324)\n",
      "('recon_loss:', 0.110115185379982, 'disc_loss:', 0.7050186395645142, 'gen_loss: ', 0.2319505214691162)\n",
      "('recon_loss:', 0.1098693460226059, 'disc_loss:', 0.7537629008293152, 'gen_loss: ', 0.2189212441444397)\n",
      "('recon_loss:', 0.0978165790438652, 'disc_loss:', 0.7649911642074585, 'gen_loss: ', 0.21958868205547333)\n",
      "('recon_loss:', 0.09813617169857025, 'disc_loss:', 0.7509593963623047, 'gen_loss: ', 0.22278358042240143)\n",
      "('recon_loss:', 0.10397599637508392, 'disc_loss:', 0.8710319399833679, 'gen_loss: ', 0.21511438488960266)\n",
      "('recon_loss:', 0.11482542753219604, 'disc_loss:', 0.8076139092445374, 'gen_loss: ', 0.24203802645206451)\n",
      "('recon_loss:', 0.0952284038066864, 'disc_loss:', 0.8510791659355164, 'gen_loss: ', 0.20903220772743225)\n",
      "('recon_loss:', 0.10226094722747803, 'disc_loss:', 0.7527371048927307, 'gen_loss: ', 0.24225465953350067)\n",
      "('recon_loss:', 0.09979689121246338, 'disc_loss:', 0.7148139476776123, 'gen_loss: ', 0.2503345310688019)\n",
      "('recon_loss:', 0.09277650713920593, 'disc_loss:', 0.7430545687675476, 'gen_loss: ', 0.2513088285923004)\n",
      "('recon_loss:', 0.10706888884305954, 'disc_loss:', 0.8222318291664124, 'gen_loss: ', 0.2557356059551239)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 0.10664864629507065, 'disc_loss:', 0.8003106713294983, 'gen_loss: ', 0.23962777853012085)\n",
      "('recon_loss:', 0.08728265017271042, 'disc_loss:', 0.7678747773170471, 'gen_loss: ', 0.26243165135383606)\n"
     ]
    }
   ],
   "source": [
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tn, fp, fn, tp: ', array([3375,    0,   48,    0]))\n"
     ]
    }
   ],
   "source": [
    "train_encoded_np = train_encoded.data.cpu().numpy()\n",
    "#train_encoded_np = np.tile(train_encoded_np,(len(train_encoded_np),1))\n",
    "dist_active = la.norm(train_encoded_np - np.tile(p_active,(no_examples,1)),axis=1)\n",
    "dist_inactive = la.norm(train_encoded_np - np.tile(p_inactive,(no_examples,1)),axis=1)\n",
    "d = dist_active - dist_inactive\n",
    "pred_labels = np.zeros((no_examples,1))\n",
    "pred_labels[d>=0] = 1\n",
    "cf = metrics.confusion_matrix(y_pred=pred_labels,y_true=labels2)\n",
    "print('tn, fp, fn, tp: ',cf.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
