{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "from torch.autograd import Variable\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "import random\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])\n",
    "train_set = dset.MNIST('/home/daiict/Desktop/udit/C-GAN./data' ,train=True, download= True,\n",
    "                       transform = transform)\n",
    "test_set = dset.MNIST('/home/daiict/Desktop/udit/C-GAN./data' ,train=False, download=True,\n",
    "                       transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469 79\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size,shuffle=True)\n",
    "print len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ip_dim = 28*28\n",
    "z_dim = 100\n",
    "comb_dim = ip_dim + z_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing BIGan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(ip_dim,500)\n",
    "        self.l2 = nn.Linear(500,500)\n",
    "        self.l3 = nn.Linear(500,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.sigmoid(self.l3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,500)\n",
    "        self.l2 = nn.Linear(500,500)\n",
    "        self.l3 = nn.Linear(500,ip_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.sigmoid(self.l3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.l1 = nn.Linear(comb_dim,500)\n",
    "        self.l2 = nn.Linear(500,500)\n",
    "        self.l3 = nn.Linear(500,500)\n",
    "        self.l4 = nn.Linear(500,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.sigmoid(self.l3(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = decoder()\n",
    "E = encoder()\n",
    "D = disc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_grad():\n",
    "    G.zero_grad()\n",
    "    E.zero_grad()\n",
    "    D.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_solver = optim.Adam(E.parameters())\n",
    "G_solver = optim.Adam(G.parameters())\n",
    "D_solver = optim.Adam(D.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.337264060974121, 0.7277527451515198)\n",
      "(10.145585060119629, 0.7138038873672485)\n",
      "(9.789278984069824, 0.691308856010437)\n",
      "(9.21243667602539, 0.658344030380249)\n",
      "(8.350910186767578, 0.6090901494026184)\n",
      "(7.082808494567871, 0.5496007204055786)\n",
      "(5.69473123550415, 0.4792934060096741)\n",
      "(3.9030611515045166, 0.43395328521728516)\n",
      "(2.618997573852539, 0.3910555839538574)\n",
      "(1.819320797920227, 0.38919198513031006)\n",
      "(1.4957300424575806, 0.40578693151474)\n",
      "(1.4750151634216309, 0.3741311728954315)\n",
      "(1.5595526695251465, 0.37664058804512024)\n",
      "(1.6713974475860596, 0.3798792362213135)\n",
      "(1.827377438545227, 0.3661670982837677)\n",
      "(2.02563738822937, 0.3665293753147125)\n",
      "(2.203314781188965, 0.36161327362060547)\n",
      "(2.2350850105285645, 0.36311689019203186)\n",
      "(2.295563220977783, 0.36514702439308167)\n",
      "(2.4512085914611816, 0.3619905412197113)\n",
      "(2.4757635593414307, 0.3582412600517273)\n",
      "(2.4274418354034424, 0.36339032649993896)\n",
      "(2.385197401046753, 0.35691359639167786)\n",
      "(2.419956684112549, 0.3467840850353241)\n",
      "(2.2722628116607666, 0.35141614079475403)\n",
      "(2.2223591804504395, 0.3537280261516571)\n",
      "(2.204881191253662, 0.3525533974170685)\n",
      "(2.140688896179199, 0.3520447015762329)\n",
      "(2.110412120819092, 0.34942108392715454)\n",
      "(2.0857174396514893, 0.3512518107891083)\n",
      "(2.0886826515197754, 0.34735366702079773)\n",
      "(1.9699387550354004, 0.35523927211761475)\n",
      "(2.0655252933502197, 0.35320770740509033)\n",
      "(2.1000781059265137, 0.3472326695919037)\n",
      "(2.135956048965454, 0.3573133051395416)\n",
      "(2.090963125228882, 0.3465280532836914)\n",
      "(2.1444270610809326, 0.3445577025413513)\n",
      "(2.2102508544921875, 0.34997859597206116)\n",
      "(2.2054781913757324, 0.35265254974365234)\n",
      "(2.1239030361175537, 0.34636324644088745)\n",
      "(2.1634604930877686, 0.34986260533332825)\n",
      "(2.2103049755096436, 0.3475733995437622)\n",
      "(2.2056713104248047, 0.34893998503685)\n",
      "(2.108248710632324, 0.34338581562042236)\n",
      "(2.226364850997925, 0.3524097800254822)\n",
      "(2.1258857250213623, 0.35119393467903137)\n",
      "(2.067357301712036, 0.3503623306751251)\n",
      "(2.2236452102661133, 0.3437691032886505)\n",
      "(2.079214096069336, 0.34768179059028625)\n",
      "(2.1287434101104736, 0.3560991585254669)\n",
      "(2.2327754497528076, 0.35025554895401)\n",
      "(2.1315388679504395, 0.35663819313049316)\n",
      "(2.1312155723571777, 0.34715503454208374)\n",
      "(2.177663803100586, 0.34981730580329895)\n",
      "(2.127971887588501, 0.3463388979434967)\n",
      "(2.150136709213257, 0.34747254848480225)\n",
      "(2.215589761734009, 0.3384399712085724)\n",
      "(2.1777024269104004, 0.3443531394004822)\n",
      "(2.191389560699463, 0.35061538219451904)\n",
      "(2.106689214706421, 0.3579776883125305)\n",
      "(2.111083745956421, 0.34890809655189514)\n",
      "(2.2016658782958984, 0.34648409485816956)\n",
      "(2.105708122253418, 0.34425681829452515)\n",
      "(2.0474236011505127, 0.35275721549987793)\n",
      "(2.0689570903778076, 0.3512338399887085)\n",
      "(2.1025445461273193, 0.3412189483642578)\n",
      "(2.1864001750946045, 0.3462037444114685)\n",
      "(2.1343414783477783, 0.34597402811050415)\n",
      "(2.0640475749969482, 0.35196346044540405)\n",
      "(2.104560375213623, 0.3555271625518799)\n",
      "(2.112145185470581, 0.3474977910518646)\n",
      "(2.158869743347168, 0.349279522895813)\n",
      "(2.1777584552764893, 0.349311888217926)\n",
      "(2.185081720352173, 0.3527127206325531)\n",
      "(2.2053987979888916, 0.35134726762771606)\n",
      "(2.2007226943969727, 0.3457294702529907)\n",
      "(2.1540310382843018, 0.3475520610809326)\n",
      "(2.131291151046753, 0.3490723967552185)\n",
      "(2.227938652038574, 0.3484385311603546)\n",
      "(2.1449530124664307, 0.34157848358154297)\n",
      "(2.0926010608673096, 0.35436198115348816)\n",
      "(2.0271008014678955, 0.349488228559494)\n",
      "(2.101505994796753, 0.3472025990486145)\n",
      "(2.165252208709717, 0.34198689460754395)\n",
      "(2.1364238262176514, 0.34276577830314636)\n",
      "(2.173415184020996, 0.34994614124298096)\n",
      "(2.1075985431671143, 0.3432238698005676)\n",
      "(2.1063232421875, 0.3423135280609131)\n",
      "(2.0785715579986572, 0.34395280480384827)\n",
      "(2.116572380065918, 0.3488980233669281)\n",
      "(2.1055657863616943, 0.34974294900894165)\n",
      "(2.029566526412964, 0.3538212776184082)\n",
      "(2.074481725692749, 0.3481189012527466)\n",
      "(2.2073614597320557, 0.34468093514442444)\n",
      "(2.153001308441162, 0.34716930985450745)\n",
      "(2.1378328800201416, 0.3458453416824341)\n",
      "(2.126711845397949, 0.35239577293395996)\n",
      "(2.1630256175994873, 0.34196236729621887)\n",
      "(2.1668753623962402, 0.34102746844291687)\n",
      "(2.1367321014404297, 0.3509831428527832)\n",
      "(2.1427202224731445, 0.3465464115142822)\n",
      "(2.1297295093536377, 0.33882269263267517)\n",
      "(2.1064071655273438, 0.34506598114967346)\n",
      "(2.0817010402679443, 0.34982219338417053)\n",
      "(2.124974012374878, 0.341549813747406)\n",
      "(2.1627655029296875, 0.3448856472969055)\n",
      "(2.0728235244750977, 0.34609970450401306)\n",
      "(2.0799765586853027, 0.34816575050354004)\n",
      "(2.0851619243621826, 0.3397284746170044)\n",
      "(2.0931785106658936, 0.3451422452926636)\n",
      "(2.068141460418701, 0.35363197326660156)\n",
      "(2.0861570835113525, 0.340536504983902)\n",
      "(2.1374330520629883, 0.35190826654434204)\n",
      "(2.1371264457702637, 0.35188794136047363)\n",
      "(2.1581575870513916, 0.3508150577545166)\n",
      "(2.229005813598633, 0.34056609869003296)\n",
      "(2.1590640544891357, 0.34645190834999084)\n",
      "(2.1479084491729736, 0.3486829698085785)\n",
      "(2.133185386657715, 0.34757500886917114)\n",
      "(2.1701996326446533, 0.3457144796848297)\n",
      "(2.1960737705230713, 0.3419901430606842)\n",
      "(2.2076539993286133, 0.34123674035072327)\n",
      "(2.114989995956421, 0.3431086242198944)\n",
      "(2.083757162094116, 0.3482978641986847)\n",
      "(2.060593366622925, 0.34749698638916016)\n",
      "(2.120819568634033, 0.34125176072120667)\n",
      "(2.0371203422546387, 0.3436945676803589)\n",
      "(2.0388433933258057, 0.34922805428504944)\n",
      "(2.0476796627044678, 0.34125980734825134)\n",
      "(2.1270039081573486, 0.3528330326080322)\n",
      "(2.097324848175049, 0.3462372422218323)\n",
      "(2.192357301712036, 0.35678336024284363)\n",
      "(2.1411945819854736, 0.34841465950012207)\n",
      "(2.175791025161743, 0.34281644225120544)\n",
      "(2.2542121410369873, 0.3397880792617798)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-99618af66934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxhat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcost2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcost2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mG_solver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for ep in range(epochs):\n",
    "    for i,(x,label) in enumerate(train_loader):\n",
    "        \n",
    "        x = Variable(x)\n",
    "        x = x.view(batch_size,28*28)\n",
    "    \n",
    "        zhat = E(x)\n",
    "        z = Variable(torch.rand(zhat.size()))\n",
    "        \n",
    "        xhat = G(z)\n",
    "        v1 = torch.cat([x,zhat],1)\n",
    "        \n",
    "        target = Variable(torch.cat([xhat,z],1).data,requires_grad = False)\n",
    "        \n",
    "        loss = nn.BCELoss()\n",
    "        cost1 = loss(v1,target)\n",
    "        cost1.backward()\n",
    "        \n",
    "        E_solver.step()\n",
    "        D_solver.step()\n",
    "        \n",
    "        clear_grad()\n",
    "        \n",
    "        zhat = E(x)\n",
    "        z = Variable(torch.rand(zhat.size()))\n",
    "        xhat = G(z)\n",
    "        target = Variable(torch.cat([x,zhat],1).data,requires_grad = False)\n",
    "        v1 = torch.cat([xhat,z],1)\n",
    "        cost2 = nn.BCELoss()(v1,target)\n",
    "        cost2.backward()\n",
    "        \n",
    "        G_solver.step()\n",
    "        D_solver.step()\n",
    "        \n",
    "        clear_grad()\n",
    "        \n",
    "        if(ep%15==0):\n",
    "            print(cost1.data[0],cost2.data[0])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
