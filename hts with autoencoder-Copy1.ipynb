{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('screen_info.txt','rb') as fl:\n",
    "    t = pickle.load(fl)\n",
    "fnames = t[0]\n",
    "totf = t[1]\n",
    "binf = t[2]\n",
    "runfile = 4\n",
    "fname = fnames[runfile]\n",
    "bf = binf[runfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/bioassay-datasets/'\n",
    "p_fingerprints = []\n",
    "c_fingerprints = []\n",
    "labels = []\n",
    "with open(path+fname+'red_train.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints.append(row[:bf-1])\n",
    "        c_fingerprints.append(row[bf:-1])\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47831, 122)\n",
      "['0' '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "print(p_fingerprints.shape)\n",
    "print(p_fingerprints[1:5,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47830, 122)\n",
      "('total no of 1s', 256139)\n",
      "('total no of 0s', 5579121)\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "p_fingerprints = p_fingerprints.astype(int)\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples , ip_dim) = p_fingerprints.shape\n",
    "labels = labels[1:]\n",
    "print(no_examples,ip_dim)\n",
    "print(\"total no of 1s\",np.sum(p_fingerprints))\n",
    "print(\"total no of 0s\",no_examples*ip_dim-np.sum(p_fingerprints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fingerprints[(p_fingerprints==0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = np.zeros((len(labels),1))\n",
    "for i,l in enumerate(labels):\n",
    "    if l=='Active':\n",
    "        labels2[i] = 1\n",
    "    else:\n",
    "        labels2[i] = 0\n",
    "labels2 = labels2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\n"
     ]
    }
   ],
   "source": [
    "no_active_ele = (sum(labels2))\n",
    "print(no_active_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = ip_dim\n",
    "h1_dim = 500\n",
    "h2_dim = 500\n",
    "h3_dim = 500\n",
    "z_dim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size):\n",
    "    samples = np.random.randint(low=0,high=no_examples,size=(batch_size,1))\n",
    "    train_batch = p_fingerprints[samples].reshape(batch_size,ip_dim)\n",
    "    train_batch = train_batch.astype(int)\n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    target = Variable(torch.cuda.FloatTensor(labels2[samples]),requires_grad=False)\n",
    "    \n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(X_dim,h1_dim)\n",
    "        self.l2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h3_dim)\n",
    "        self.l4 = nn.Linear(h3_dim,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,h3_dim)\n",
    "        self.l2 = nn.Linear(h3_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h1_dim)\n",
    "        self.l4 = nn.Linear(h1_dim,X_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.tanh(self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim+2,500)\n",
    "        self.lin2 = nn.Linear(500,100)\n",
    "        self.lin3 = nn.Linear(100,100)\n",
    "        self.lin4 = nn.Linear(100,30)\n",
    "        self.lin5 = nn.Linear(30,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.lin1(x))\n",
    "        x = F.selu(self.lin2(x))\n",
    "        x = F.selu(self.lin3(x))\n",
    "        x = F.selu(self.lin4(x))\n",
    "        x = F.sigmoid(self.lin5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label_info(y,batch_size):\n",
    "\n",
    "    tmp = np.zeros((batch_size,2))\n",
    "    tmp2 = np.zeros((batch_size,1))\n",
    "    y = y.cpu().data.numpy().reshape(batch_size,1)\n",
    "    tmp2[y==0] = 5\n",
    "    tmp3 = np.zeros((batch_size,1))\n",
    "    tmp3[y==1] = 5\n",
    "    tmp = np.concatenate((tmp2,tmp3),1)\n",
    "    label_info = torch.from_numpy((tmp)).cuda()\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size):\n",
    "    \n",
    "    for it in range(3500):\n",
    "        x,y = get_train_batch(batch_size)\n",
    "        z = Q(x)\n",
    "\n",
    "        #Reconstruction\n",
    "        \n",
    "        x_recon = P(z)\n",
    "        '''\n",
    "        x_recon[x_recon<0] = 0\n",
    "        x_recon[x_recon>0] = 1\n",
    "        x_tar = Variable(torch.cuda.FloatTensor(x.size()),requires_grad=False)\n",
    "        x_tar[x==-1] = 0\n",
    "        x_tar[x==1] = 1'''\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        CEL = criterion(x_recon, x)\n",
    "        \n",
    "        CEL.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        P_solver.step()\n",
    "        \n",
    "        Q.zero_grad()\n",
    "        P.zero_grad()\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        label_info = (add_label_info(y,batch_size))\n",
    "        z_false = np.concatenate((z.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "        z_false = Variable(torch.FloatTensor(z_false)).cuda()\n",
    "        #z_false = torch.cat((z,label_info),1)\n",
    "        z_true = np.random.rand(batch_size,z_dim)\n",
    "        z_true = np.concatenate((z_true,label_info.cpu().numpy()),1)\n",
    "        z_true = Variable(torch.FloatTensor(z_true).cuda())\n",
    "        #z_true = torch.cat((z_true,label_info),1)\n",
    "        z_true_op = Variable(D(z_true).data,requires_grad=False)\n",
    "        \n",
    "        z_false_op = D(z_false)\n",
    "        add_small = 1e-20\n",
    "        \n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss_d = criterion(z_false_op,z_true_op)\n",
    "        #loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        D_solver.step()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(it%50==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', CEL.data[0],'disc_loss:', loss_d.data[0],'gen_loss: ',G_loss.data[0])\n",
    "            #print(x_recon[0][:50].cpu().data.numpy().T)\n",
    "            #print()\n",
    "            #print(x[0][:50].cpu().data.numpy().T)\n",
    "           # print()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    Q = encoder()\n",
    "    Q.cuda()\n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-4)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-5)\n",
    "    P = decoder()\n",
    "    P.cuda()\n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-4)\n",
    "    D = disc()\n",
    "    D.cuda()\n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-3)\n",
    "    batch_size = 120\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size)\n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 0.9942927360534668, 'disc_loss:', 0.6853199005126953, 'gen_loss: ', 0.8057406544685364)\n",
      "('recon_loss:', 0.1403856724500656, 'disc_loss:', 0.012377756647765636, 'gen_loss: ', 0.00014813653251621872)\n",
      "('recon_loss:', 0.14554402232170105, 'disc_loss:', 0.018647173419594765, 'gen_loss: ', 0.004062002059072256)\n",
      "('recon_loss:', 0.15403220057487488, 'disc_loss:', 0.023720065131783485, 'gen_loss: ', 0.0022146902047097683)\n",
      "('recon_loss:', 0.13927140831947327, 'disc_loss:', 0.010000701062381268, 'gen_loss: ', 0.0010849550599232316)\n",
      "('recon_loss:', 0.13658882677555084, 'disc_loss:', 0.011991172097623348, 'gen_loss: ', 0.0010550322476774454)\n",
      "('recon_loss:', 0.12224192172288895, 'disc_loss:', 0.008961254730820656, 'gen_loss: ', 0.0006800384726375341)\n",
      "('recon_loss:', 0.1173381358385086, 'disc_loss:', 0.03828955069184303, 'gen_loss: ', 0.0012421953724697232)\n",
      "('recon_loss:', 0.11348015069961548, 'disc_loss:', 0.008183091878890991, 'gen_loss: ', 0.0006673209136351943)\n",
      "('recon_loss:', 0.09766305238008499, 'disc_loss:', 0.006860694382339716, 'gen_loss: ', 0.00014872124302200973)\n",
      "('recon_loss:', 0.08312740176916122, 'disc_loss:', 0.028743069618940353, 'gen_loss: ', 0.004279411397874355)\n",
      "('recon_loss:', 0.08431105315685272, 'disc_loss:', 0.006219223141670227, 'gen_loss: ', 0.001064421609044075)\n",
      "('recon_loss:', 0.09198017418384552, 'disc_loss:', 0.009214804507791996, 'gen_loss: ', 0.0006330367759801447)\n",
      "('recon_loss:', 0.07287164032459259, 'disc_loss:', 0.010421284474432468, 'gen_loss: ', 0.0006559109897352755)\n",
      "('recon_loss:', 0.06526266038417816, 'disc_loss:', 0.00933681707829237, 'gen_loss: ', 0.000654742878396064)\n",
      "('recon_loss:', 0.0685078501701355, 'disc_loss:', 0.008753188885748386, 'gen_loss: ', 0.0005655980785377324)\n",
      "('recon_loss:', 0.06821080297231674, 'disc_loss:', 0.008235329762101173, 'gen_loss: ', 0.0004995320923626423)\n",
      "('recon_loss:', 0.06159358471632004, 'disc_loss:', 0.006209635175764561, 'gen_loss: ', 0.00041854119626805186)\n",
      "('recon_loss:', 0.06632039695978165, 'disc_loss:', 0.009832018986344337, 'gen_loss: ', 0.007495556958019733)\n",
      "('recon_loss:', 0.059028707444667816, 'disc_loss:', 0.008612455800175667, 'gen_loss: ', 0.00813905056566)\n",
      "('recon_loss:', 0.05371328815817833, 'disc_loss:', 0.004543688613921404, 'gen_loss: ', 0.0002715395821724087)\n",
      "('recon_loss:', 0.04948331043124199, 'disc_loss:', 0.0045415740460157394, 'gen_loss: ', 0.0002504510630387813)\n",
      "('recon_loss:', 0.047836944460868835, 'disc_loss:', 0.00425849761813879, 'gen_loss: ', 0.00022223306586965919)\n",
      "('recon_loss:', 0.04290924593806267, 'disc_loss:', 0.004314951598644257, 'gen_loss: ', 0.00024147411750163883)\n",
      "('recon_loss:', 0.050432950258255005, 'disc_loss:', 0.004034299403429031, 'gen_loss: ', 0.00020170651259832084)\n",
      "('recon_loss:', 0.05047772079706192, 'disc_loss:', 0.003657688619568944, 'gen_loss: ', 0.0001982456597033888)\n",
      "('recon_loss:', 0.0641695037484169, 'disc_loss:', 0.003543886123225093, 'gen_loss: ', 0.00016513610898982733)\n",
      "('recon_loss:', 0.04400499537587166, 'disc_loss:', 0.009396021254360676, 'gen_loss: ', 0.005513404030352831)\n",
      "('recon_loss:', 0.04474068433046341, 'disc_loss:', 0.007838461548089981, 'gen_loss: ', 0.006573291961103678)\n",
      "('recon_loss:', 0.0387362577021122, 'disc_loss:', 0.0038793047424405813, 'gen_loss: ', 9.226216934621334e-05)\n",
      "('recon_loss:', 0.06241244822740555, 'disc_loss:', 0.0337575227022171, 'gen_loss: ', 0.00116890633944422)\n",
      "('recon_loss:', 0.04873395711183548, 'disc_loss:', 0.017392724752426147, 'gen_loss: ', 0.0006594547303393483)\n",
      "('recon_loss:', 0.043705862015485764, 'disc_loss:', 0.02709147147834301, 'gen_loss: ', 0.0010777382412925363)\n",
      "('recon_loss:', 0.029803922399878502, 'disc_loss:', 0.03817904368042946, 'gen_loss: ', 0.0013026334345340729)\n",
      "('recon_loss:', 0.04085316136479378, 'disc_loss:', 0.05049153417348862, 'gen_loss: ', 0.0052535091526806355)\n",
      "('recon_loss:', 0.039953313767910004, 'disc_loss:', 0.03973953425884247, 'gen_loss: ', 0.00844859890639782)\n",
      "('recon_loss:', 0.031653229147195816, 'disc_loss:', 0.013072057627141476, 'gen_loss: ', 0.0006654023891314864)\n",
      "('recon_loss:', 0.04122992232441902, 'disc_loss:', 0.011242395266890526, 'gen_loss: ', 0.0007359969895333052)\n",
      "('recon_loss:', 0.036670930683612823, 'disc_loss:', 0.010823831893503666, 'gen_loss: ', 0.0006853836239315569)\n",
      "('recon_loss:', 0.03714520111680031, 'disc_loss:', 0.011433715932071209, 'gen_loss: ', 0.0007544411928392947)\n",
      "('recon_loss:', 0.03496108576655388, 'disc_loss:', 0.006570807192474604, 'gen_loss: ', 0.0002647956134751439)\n",
      "('recon_loss:', 0.030883673578500748, 'disc_loss:', 0.038527823984622955, 'gen_loss: ', 0.006221694406121969)\n",
      "('recon_loss:', 0.035697367042303085, 'disc_loss:', 0.083583764731884, 'gen_loss: ', 0.0044240280985832214)\n",
      "('recon_loss:', 0.04049467667937279, 'disc_loss:', 0.05534819886088371, 'gen_loss: ', 0.005489868577569723)\n",
      "('recon_loss:', 0.033712953329086304, 'disc_loss:', 0.05778595432639122, 'gen_loss: ', 0.005645825527608395)\n",
      "('recon_loss:', 0.02885676734149456, 'disc_loss:', 0.028710395097732544, 'gen_loss: ', 0.0013138495851308107)\n",
      "('recon_loss:', 0.029870275408029556, 'disc_loss:', 0.022289933636784554, 'gen_loss: ', 0.0014796692412346601)\n",
      "('recon_loss:', 0.026489270851016045, 'disc_loss:', 0.033724188804626465, 'gen_loss: ', 0.0027692343574017286)\n",
      "('recon_loss:', 0.033361438661813736, 'disc_loss:', 0.026027048006653786, 'gen_loss: ', 0.0020821494981646538)\n",
      "('recon_loss:', 0.030408278107643127, 'disc_loss:', 0.010930346325039864, 'gen_loss: ', 0.0005463000852614641)\n",
      "('recon_loss:', 0.029246583580970764, 'disc_loss:', 0.013746337033808231, 'gen_loss: ', 0.0008689230890013278)\n",
      "('recon_loss:', 0.03277141600847244, 'disc_loss:', 0.01340855285525322, 'gen_loss: ', 0.0008941065170802176)\n",
      "('recon_loss:', 0.022046111524105072, 'disc_loss:', 0.018168218433856964, 'gen_loss: ', 0.001409543096087873)\n",
      "('recon_loss:', 0.021266400814056396, 'disc_loss:', 0.011856638826429844, 'gen_loss: ', 0.0008066730806604028)\n",
      "('recon_loss:', 0.020560970529913902, 'disc_loss:', 0.011493257246911526, 'gen_loss: ', 0.0007375322165898979)\n",
      "('recon_loss:', 0.02908085472881794, 'disc_loss:', 0.011101827025413513, 'gen_loss: ', 0.0007345753838308156)\n",
      "('recon_loss:', 0.02237444743514061, 'disc_loss:', 0.010311717167496681, 'gen_loss: ', 0.0006800580304116011)\n",
      "('recon_loss:', 0.027267292141914368, 'disc_loss:', 0.01049830298870802, 'gen_loss: ', 0.0006916368147358298)\n",
      "('recon_loss:', 0.026575246825814247, 'disc_loss:', 0.009502728469669819, 'gen_loss: ', 0.0005964728188700974)\n",
      "('recon_loss:', 0.02293011173605919, 'disc_loss:', 0.009302153252065182, 'gen_loss: ', 0.000574730453081429)\n",
      "('recon_loss:', 0.02512190118432045, 'disc_loss:', 0.009162096306681633, 'gen_loss: ', 0.000579122337512672)\n",
      "('recon_loss:', 0.025931842625141144, 'disc_loss:', 0.008706999011337757, 'gen_loss: ', 0.0005629144725389779)\n",
      "('recon_loss:', 0.022383764386177063, 'disc_loss:', 0.008428354747593403, 'gen_loss: ', 0.0005283022182993591)\n",
      "('recon_loss:', 0.019716251641511917, 'disc_loss:', 0.00808949489146471, 'gen_loss: ', 0.0004887499962933362)\n",
      "('recon_loss:', 0.018665678799152374, 'disc_loss:', 0.0078629981726408, 'gen_loss: ', 0.00047311378875747323)\n",
      "('recon_loss:', 0.024108339101076126, 'disc_loss:', 0.010688806883990765, 'gen_loss: ', 0.0008244272321462631)\n",
      "('recon_loss:', 0.01713063195347786, 'disc_loss:', 0.010008290410041809, 'gen_loss: ', 0.0008764027734287083)\n",
      "('recon_loss:', 0.019988102838397026, 'disc_loss:', 0.007284772582352161, 'gen_loss: ', 0.0004433407448232174)\n",
      "('recon_loss:', 0.01916121318936348, 'disc_loss:', 0.007168612442910671, 'gen_loss: ', 0.0004440096963662654)\n",
      "('recon_loss:', 0.02118408866226673, 'disc_loss:', 0.007008697371929884, 'gen_loss: ', 0.0004306126502342522)\n"
     ]
    }
   ],
   "source": [
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_path = os.getcwd() + '/model_enc_' + str(fname)\n",
    "torch.save(Q.state_dict(),encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450.805234909\n"
     ]
    }
   ],
   "source": [
    "#entire_batch,batch_labels = get_train_batch(no_examples)\n",
    "tic = time.time()\n",
    "## It takes too much memory. Split in chunks and \n",
    "z_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "toc = time.time()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_new_z = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate new samples from orignal ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if generate_new_z == True:\n",
    "    n_comb = 5\n",
    "    n_samples = 100\n",
    "    extra_samples = torch.cuda.FloatTensor(n_samples,z_dim)\n",
    "    extra_labels = Variable(torch.ones(n_samples).cuda())\n",
    "    for i in range(n_samples):\n",
    "        #coeff = np.random.rand(n_comb,1)\n",
    "        coeff = Variable(torch.randn(n_comb,1)).cuda()\n",
    "        active_z_encoded = z_encoded[torch.cuda.FloatTensor(labels2)==1]\n",
    "        tmp_rand_nos = torch.randperm(int(no_active_ele))\n",
    "        rand_nos = tmp_rand_nos[0:n_comb].cuda()\n",
    "        rand_z = torch.transpose(z_encoded[rand_nos],0,1)\n",
    "        extra_samples[i] = torch.cuda.FloatTensor(torch.matmul(rand_z,coeff).data)\n",
    "    extra_samples = Variable(extra_samples)\n",
    "    \n",
    "    new_z_encoded = torch.cat((z_encoded,extra_samples),0)\n",
    "    new_labels = torch.cat((Variable(torch.cuda.FloatTensor(labels2)),extra_labels),0)\n",
    "    perm = torch.randperm(no_examples+n_samples).cuda()\n",
    "    new_z_encoded = new_z_encoded[perm]\n",
    "    new_labels = new_labels[perm]\n",
    "    batch_labels_np = new_labels.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_encoded = z_encoded.cpu().data.numpy()[:,0]\n",
    "# y_encoded = z_encoded.cpu().data.numpy()[:,1]\n",
    "# w_encoded = z_encoded.cpu().data.numpy()[:,2]\n",
    "\n",
    "# # batch_labels_np = batch_labels_np.astype(int)\n",
    "# # print(batch_labels_np.dtype)\n",
    "# # print(batch_labels_np.shape)\n",
    "# batch_labels_np = list(labels2)\n",
    "\n",
    "# colors = []\n",
    "# for l in batch_labels_np:\n",
    "#     colors.append(\"C\"+str(int(l)))\n",
    "    \n",
    "# #plt.scatter(x_encoded,y_encoded,c=colors)\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(x_encoded,y_encoded,w_encoded,c=colors)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(size):\n",
    "    if generate_new_z == True:\n",
    "        print(\"gng here\")\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples+n_samples)[:size].numpy())\n",
    "        return new_z_encoded[ind], new_labels[ind]\n",
    "    else:\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples)[:size].numpy())\n",
    "        return z_encoded[ind], Variable(torch.cuda.LongTensor(labels2)[ind],requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,800)\n",
    "        self.l2 = nn.Linear(800,500)\n",
    "        self.l3 = nn.Linear(500,400)\n",
    "        self.l4 = nn.Linear(400,70)\n",
    "        self.l5 = nn.Linear(70,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.l1(x))\n",
    "        x = F.selu(self.l2(x))\n",
    "        x = F.selu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = (self.l5(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_disc_model(w):\n",
    "    d = Discriminator().cuda()\n",
    "    d_optim = optim.Adam(d.parameters(),lr=1e-4)\n",
    "    d = train_disc(d,d_optim,w)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_disc(d,d_optim,w):\n",
    "    for ep in range(2000):\n",
    "        d_optim.zero_grad()\n",
    "        x,true_l = sample_z(200)\n",
    "        true_l = true_l.view(true_l.size()[0],)\n",
    "        p_labels = d(x)\n",
    "        weights = torch.Tensor([1,w]).cuda()\n",
    "        criteria = nn.CrossEntropyLoss(weight=weights)\n",
    "        true_l = true_l.type(torch.cuda.LongTensor)\n",
    "        loss = criteria(p_labels,true_l)\n",
    "        loss.backward(retain_graph=True)\n",
    "        d_optim.step()\n",
    "        \n",
    "#         if(ep%50==49):\n",
    "#             print(loss.data[0])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.linspace(5,30,25)\n",
    "# with open(\"cnt_test_good)weights.txt\",'rb') as f:\n",
    "#     weights = pickle.load(f)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if generate_new_z == True:\n",
    "#     train_encoded = (new_z_encoded)\n",
    "#     labels_final = batch_labels_np\n",
    "# else:\n",
    "train_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "labels_final = labels2\n",
    "fn_min  = 48\n",
    "    \n",
    "cm_autoencoder = []\n",
    "cm_autoencoder.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w: ', 5.0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6692cdd596d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_disc_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_final\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-11bd8b6845f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mselu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSELU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/thnn/activation.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, inplace)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         backend.ELU_updateOutput(\n\u001b[1;32m    177\u001b[0m             \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at /pytorch/torch/lib/THC/generic/THCStorage.cu:66"
     ]
    }
   ],
   "source": [
    "model_path = os.getcwd() + '/model_autoencoder_' + str(fname)\n",
    "for w in weights:\n",
    "    print(\"w: \",w)\n",
    "    d = gen_disc_model(w)\n",
    "    train_op = d(train_encoded).cpu().data.numpy()\n",
    "    train_op = np.argmax(train_op,axis=1)\n",
    "    cf = metrics.confusion_matrix(labels_final,train_op)\n",
    "    [tn, fp, fn, tp]  = cf.ravel()\n",
    "    print('tn, fp, fn, tp: ',cf.ravel())\n",
    "    if(fn < fn_min):\n",
    "        fn_min = fn\n",
    "        torch.save(d.state_dict(),model_path)\n",
    "        print(\"saving model on weight: \",w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"autoencoder_arti_ex_cm\",'wb') as f:\n",
    "    pickle.dump(cm_autoencoder,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"autoencoder_1.txt\",'wb') as fb:\n",
    "    pickle.dump(cm_autoencoder,fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
