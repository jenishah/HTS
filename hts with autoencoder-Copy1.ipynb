{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import time\n",
    "import sampling_with_data_cleaning as sdc\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('screen_info.txt','rb') as fl:\n",
    "    t = pickle.load(fl)\n",
    "fnames = t[0]\n",
    "totf = t[1]\n",
    "binf = t[2]\n",
    "runfile = 0\n",
    "fname = fnames[runfile]\n",
    "bf = binf[runfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/bioassay-datasets/'\n",
    "p_fingerprints = []\n",
    "c_fingerprints = []\n",
    "labels = []\n",
    "with open(path+fname+'red_train.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints.append(row[:bf])\n",
    "        c_fingerprints.append(row[bf:-1])\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 112)\n",
      "['0' '0' '1' '0']\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "print(p_fingerprints.shape)\n",
    "print(p_fingerprints[1:5,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3422, 112)\n",
      "('total no of 1s', 25981)\n",
      "('total no of 0s', 357283)\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "p_fingerprints = p_fingerprints.astype(int)\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples , ip_dim) = p_fingerprints.shape\n",
    "labels = labels[1:]\n",
    "print(no_examples,ip_dim)\n",
    "print(\"total no of 1s\",np.sum(p_fingerprints))\n",
    "print(\"total no of 0s\",no_examples*ip_dim-np.sum(p_fingerprints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = np.zeros((len(labels),1))\n",
    "for i,l in enumerate(labels):\n",
    "    if l=='Active':\n",
    "        labels2[i] = 1\n",
    "    else:\n",
    "        labels2[i] = 0\n",
    "labels2 = labels2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 106 new samples\n",
      "removing 61 samples\n",
      "[3422  968  967 2277 3260 2966 3090 1862 1324  770 2147  749 2145 1668  694\n",
      " 3051 2535 3311 3363 3364 3176 1049 3045 3049 2285 2290 1156 1801 3212 1554\n",
      " 1860 3064 1858 1476 2423 2422 2534  123 1284  126  125  122 1980 2483 2661\n",
      " 2441 2260 1967 2259 2012 2225  852  843 3173 3107 3367 3263 3264 3096 3281\n",
      " 3270]\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints,labels2 = sdc.clean_data(p_fingerprints,labels2)\n",
    "no_examples,ip_dim = p_fingerprints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 154.]\n"
     ]
    }
   ],
   "source": [
    "no_active_ele = (sum(labels2))\n",
    "print(no_active_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = ip_dim\n",
    "h1_dim = 500\n",
    "h2_dim = 500\n",
    "h3_dim = 500\n",
    "z_dim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size):\n",
    "    samples = np.random.randint(low=0,high=no_examples,size=(batch_size,1))\n",
    "    train_batch = p_fingerprints[samples].reshape(batch_size,ip_dim)\n",
    "    train_batch = train_batch.astype(int)\n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    target = Variable(torch.cuda.FloatTensor(labels2[samples]),requires_grad=False)\n",
    "    \n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(X_dim,h1_dim)\n",
    "        self.l2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h3_dim)\n",
    "        self.l4 = nn.Linear(h3_dim,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,h3_dim)\n",
    "        self.l2 = nn.Linear(h3_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h1_dim)\n",
    "        self.l4 = nn.Linear(h1_dim,X_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.tanh(self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim+2,500)\n",
    "        self.lin2 = nn.Linear(500,100)\n",
    "        #self.lin3 = nn.Linear(100,100)\n",
    "        self.lin4 = nn.Linear(100,30)\n",
    "        self.lin5 = nn.Linear(30,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.lin1(x))\n",
    "        x = F.selu(self.lin2(x))\n",
    "        #x = F.selu(self.lin3(x))\n",
    "        x = F.selu(self.lin4(x))\n",
    "        x = F.sigmoid(self.lin5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label_info(y,batch_size):\n",
    "\n",
    "    tmp = np.zeros((batch_size,2))\n",
    "    tmp2 = np.zeros((batch_size,1))\n",
    "    y = y.cpu().data.numpy().reshape(batch_size,1)\n",
    "    tmp2[y==0] = 5\n",
    "    tmp3 = np.zeros((batch_size,1))\n",
    "    tmp3[y==1] = 5\n",
    "    tmp = np.concatenate((tmp2,tmp3),1)\n",
    "    label_info = torch.from_numpy((tmp)).cuda()\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size):\n",
    "    \n",
    "    for it in range(3500):\n",
    "        x,y = get_train_batch(batch_size)\n",
    "        z = Q(x)\n",
    "\n",
    "        #Reconstruction\n",
    "        \n",
    "        x_recon = P(z)\n",
    "        '''\n",
    "        x_recon[x_recon<0] = 0\n",
    "        x_recon[x_recon>0] = 1\n",
    "        x_tar = Variable(torch.cuda.FloatTensor(x.size()),requires_grad=False)\n",
    "        x_tar[x==-1] = 0\n",
    "        x_tar[x==1] = 1'''\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        CEL = criterion(x_recon, x)\n",
    "        \n",
    "        CEL.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        P_solver.step()\n",
    "        \n",
    "        Q.zero_grad()\n",
    "        P.zero_grad()\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        label_info = (add_label_info(y,batch_size))\n",
    "        z_false = np.concatenate((z.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "        z_false = Variable(torch.FloatTensor(z_false)).cuda()\n",
    "        #z_false = torch.cat((z,label_info),1)\n",
    "        z_true = np.random.rand(batch_size,z_dim)\n",
    "        z_true = np.concatenate((z_true,label_info.cpu().numpy()),1)\n",
    "        z_true = Variable(torch.FloatTensor(z_true).cuda())\n",
    "        #z_true = torch.cat((z_true,label_info),1)\n",
    "        z_true_op = Variable(D(z_true).data,requires_grad=False)\n",
    "        \n",
    "        z_false_op = D(z_false)\n",
    "        add_small = 1e-20\n",
    "        \n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss_d = criterion(z_false_op,z_true_op)\n",
    "        #loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        D_solver.step()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(it%50==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', CEL.data[0],'disc_loss:', loss_d.data[0],'gen_loss: ',G_loss.data[0])\n",
    "            #print(x_recon[0][:50].cpu().data.numpy().T)\n",
    "            #print()\n",
    "            #print(x[0][:50].cpu().data.numpy().T)\n",
    "           # print()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    Q = encoder()\n",
    "    Q.cuda()\n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-4)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-5)\n",
    "    P = decoder()\n",
    "    P.cuda()\n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-4)\n",
    "    D = disc()\n",
    "    D.cuda()\n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-3)\n",
    "    batch_size = 120\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size)\n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 0.06175412982702255, 'disc_loss:', 0.6929045915603638, 'gen_loss: ', 0.7217796444892883)\n",
      "('recon_loss:', 0.047042906284332275, 'disc_loss:', 0.00024133353144861758, 'gen_loss: ', 1.0049680895463098e-05)\n",
      "('recon_loss:', 0.03817024081945419, 'disc_loss:', 0.00020901851530652493, 'gen_loss: ', 8.547125617042184e-06)\n",
      "('recon_loss:', 0.03266599401831627, 'disc_loss:', 0.00026663977769203484, 'gen_loss: ', 1.3540688087232411e-05)\n",
      "('recon_loss:', 0.03228125348687172, 'disc_loss:', 0.00012333346239756793, 'gen_loss: ', 6.604869668080937e-06)\n",
      "('recon_loss:', 0.023817162960767746, 'disc_loss:', 0.00022105086827650666, 'gen_loss: ', 1.3725653843721375e-05)\n",
      "('recon_loss:', 0.022138474509119987, 'disc_loss:', 0.0001817665179260075, 'gen_loss: ', 1.3478643268172164e-05)\n",
      "('recon_loss:', 0.02096492238342762, 'disc_loss:', 0.00012767301814164966, 'gen_loss: ', 7.5894636211160105e-06)\n",
      "('recon_loss:', 0.016020575538277626, 'disc_loss:', 0.0001512121525593102, 'gen_loss: ', 1.4077148989599664e-05)\n",
      "('recon_loss:', 0.01591532677412033, 'disc_loss:', 8.478179370285943e-05, 'gen_loss: ', 4.156451723247301e-06)\n",
      "('recon_loss:', 0.015979692339897156, 'disc_loss:', 0.00013049451808910817, 'gen_loss: ', 8.31462148198625e-06)\n",
      "('recon_loss:', 0.01428324356675148, 'disc_loss:', 0.00011613048991421238, 'gen_loss: ', 5.196187885303516e-06)\n",
      "('recon_loss:', 0.013619037345051765, 'disc_loss:', 0.00012098903971491382, 'gen_loss: ', 5.713225618819706e-06)\n",
      "('recon_loss:', 0.01415638905018568, 'disc_loss:', 8.160833385773003e-05, 'gen_loss: ', 3.252441501899739e-06)\n",
      "('recon_loss:', 0.011497755534946918, 'disc_loss:', 7.068427657941356e-05, 'gen_loss: ', 3.3597361834836192e-06)\n",
      "('recon_loss:', 0.012399476021528244, 'disc_loss:', 0.0001586951984791085, 'gen_loss: ', 7.2907791945908684e-06)\n",
      "('recon_loss:', 0.008917467668652534, 'disc_loss:', 0.00011371310392860323, 'gen_loss: ', 4.81313509226311e-06)\n",
      "('recon_loss:', 0.010351604782044888, 'disc_loss:', 9.966318611986935e-05, 'gen_loss: ', 5.160854016139638e-06)\n",
      "('recon_loss:', 0.00940953753888607, 'disc_loss:', 6.952368130441755e-05, 'gen_loss: ', 2.643483412612113e-06)\n",
      "('recon_loss:', 0.008662753738462925, 'disc_loss:', 9.265505650546402e-05, 'gen_loss: ', 4.04820002586348e-06)\n",
      "('recon_loss:', 0.008025015704333782, 'disc_loss:', 9.086473437491804e-05, 'gen_loss: ', 3.5792995731753763e-06)\n",
      "('recon_loss:', 0.009186819195747375, 'disc_loss:', 7.356403511948884e-05, 'gen_loss: ', 3.036880571016809e-06)\n",
      "('recon_loss:', 0.008634853176772594, 'disc_loss:', 9.472117380937561e-05, 'gen_loss: ', 4.307490144128678e-06)\n",
      "('recon_loss:', 0.007249358110129833, 'disc_loss:', 6.754437345080078e-05, 'gen_loss: ', 3.341870069561992e-06)\n",
      "('recon_loss:', 0.007667009253054857, 'disc_loss:', 6.49739959044382e-05, 'gen_loss: ', 3.7312991025828524e-06)\n",
      "('recon_loss:', 0.007783291861414909, 'disc_loss:', 8.198468276532367e-05, 'gen_loss: ', 4.133625225222204e-06)\n",
      "('recon_loss:', 0.005904070101678371, 'disc_loss:', 8.778234041528776e-05, 'gen_loss: ', 3.980642759415787e-06)\n",
      "('recon_loss:', 0.006678068079054356, 'disc_loss:', 6.749275780748576e-05, 'gen_loss: ', 5.003892965760315e-06)\n",
      "('recon_loss:', 0.00633686687797308, 'disc_loss:', 9.668071288615465e-05, 'gen_loss: ', 3.556450337782735e-06)\n",
      "('recon_loss:', 0.007499157916754484, 'disc_loss:', 9.250662697013468e-05, 'gen_loss: ', 3.6935409752913984e-06)\n",
      "('recon_loss:', 0.006753597408533096, 'disc_loss:', 5.6836390285752714e-05, 'gen_loss: ', 2.13087378142518e-06)\n",
      "('recon_loss:', 0.0060092671774327755, 'disc_loss:', 5.148397394805215e-05, 'gen_loss: ', 2.273928203067044e-06)\n",
      "('recon_loss:', 0.005231953226029873, 'disc_loss:', 0.00011286391236353666, 'gen_loss: ', 3.741227828868432e-06)\n",
      "('recon_loss:', 0.005769897252321243, 'disc_loss:', 5.9146565035916865e-05, 'gen_loss: ', 3.3875671761052217e-06)\n",
      "('recon_loss:', 0.00596771202981472, 'disc_loss:', 8.868206350598484e-05, 'gen_loss: ', 2.8600504720088793e-06)\n",
      "('recon_loss:', 0.004084709566086531, 'disc_loss:', 0.000126837840070948, 'gen_loss: ', 3.823683982773218e-06)\n",
      "('recon_loss:', 0.003963549621403217, 'disc_loss:', 7.023105717962608e-05, 'gen_loss: ', 3.0070839329709997e-06)\n",
      "('recon_loss:', 0.004131312482059002, 'disc_loss:', 5.543923907680437e-05, 'gen_loss: ', 2.6454770249983994e-06)\n",
      "('recon_loss:', 0.004519469570368528, 'disc_loss:', 5.9986508858855814e-05, 'gen_loss: ', 2.9107200134603772e-06)\n",
      "('recon_loss:', 0.004139335826039314, 'disc_loss:', 9.085181227419525e-05, 'gen_loss: ', 3.333922450110549e-06)\n",
      "('recon_loss:', 0.004494588356465101, 'disc_loss:', 7.622026168974116e-05, 'gen_loss: ', 3.785937906286563e-06)\n",
      "('recon_loss:', 0.004479333758354187, 'disc_loss:', 0.00010117492638528347, 'gen_loss: ', 4.7942799028533045e-06)\n",
      "('recon_loss:', 0.005061807110905647, 'disc_loss:', 7.725627074250951e-05, 'gen_loss: ', 4.513153271545889e-06)\n",
      "('recon_loss:', 0.004816474858671427, 'disc_loss:', 8.140691352309659e-05, 'gen_loss: ', 3.3776443615352036e-06)\n",
      "('recon_loss:', 0.004748662933707237, 'disc_loss:', 7.734587416052818e-05, 'gen_loss: ', 2.8660172119998606e-06)\n",
      "('recon_loss:', 0.0041291369125247, 'disc_loss:', 5.068922837381251e-05, 'gen_loss: ', 1.844777216319926e-06)\n",
      "('recon_loss:', 0.00414013396948576, 'disc_loss:', 4.4636941311182454e-05, 'gen_loss: ', 1.8288817500433652e-06)\n",
      "('recon_loss:', 0.0038773573469370604, 'disc_loss:', 7.091130100889131e-05, 'gen_loss: ', 2.952452177851228e-06)\n",
      "('recon_loss:', 0.0029399082995951176, 'disc_loss:', 8.763667574385181e-05, 'gen_loss: ', 4.368101599538932e-06)\n",
      "('recon_loss:', 0.0030499319545924664, 'disc_loss:', 8.417441131314263e-05, 'gen_loss: ', 3.905159701389493e-06)\n",
      "('recon_loss:', 0.0036815537605434656, 'disc_loss:', 6.598786421818659e-05, 'gen_loss: ', 3.7333004456741037e-06)\n",
      "('recon_loss:', 0.002765617100521922, 'disc_loss:', 9.310953464591876e-05, 'gen_loss: ', 4.338293820183026e-06)\n",
      "('recon_loss:', 0.0031449447851628065, 'disc_loss:', 5.889719250262715e-05, 'gen_loss: ', 2.730916321525001e-06)\n",
      "('recon_loss:', 0.0034565269015729427, 'disc_loss:', 4.935772085445933e-05, 'gen_loss: ', 3.008088469869108e-06)\n",
      "('recon_loss:', 0.003391420468688011, 'disc_loss:', 6.133524584583938e-05, 'gen_loss: ', 2.09909671866626e-06)\n",
      "('recon_loss:', 0.003013408975675702, 'disc_loss:', 7.322178862523288e-05, 'gen_loss: ', 3.3597634683246724e-06)\n",
      "('recon_loss:', 0.002676746342331171, 'disc_loss:', 5.80008709221147e-05, 'gen_loss: ', 2.373280722167692e-06)\n",
      "('recon_loss:', 0.002552944468334317, 'disc_loss:', 5.126811083755456e-05, 'gen_loss: ', 2.6196551061730133e-06)\n",
      "('recon_loss:', 0.0027260154020041227, 'disc_loss:', 9.264584514312446e-05, 'gen_loss: ', 3.465070903985179e-06)\n",
      "('recon_loss:', 0.0028980555944144726, 'disc_loss:', 7.818656013114378e-05, 'gen_loss: ', 3.4233385122206528e-06)\n",
      "('recon_loss:', 0.003532692324370146, 'disc_loss:', 3.9957121771294624e-05, 'gen_loss: ', 1.9242545477027306e-06)\n",
      "('recon_loss:', 0.0031179101206362247, 'disc_loss:', 3.0669656553072855e-05, 'gen_loss: ', 1.3351444749787333e-06)\n",
      "('recon_loss:', 0.0027580023743212223, 'disc_loss:', 7.287760672625154e-05, 'gen_loss: ', 2.3395082280330826e-06)\n",
      "('recon_loss:', 0.0027245504315942526, 'disc_loss:', 4.8721158236730844e-05, 'gen_loss: ', 1.6848370023581083e-06)\n",
      "('recon_loss:', 0.002680446021258831, 'disc_loss:', 0.00010511897562537342, 'gen_loss: ', 3.452152213867521e-06)\n",
      "('recon_loss:', 0.002683026483282447, 'disc_loss:', 3.8153219065861776e-05, 'gen_loss: ', 2.6375253128207987e-06)\n",
      "('recon_loss:', 0.0027447782922536135, 'disc_loss:', 3.6954956158297136e-05, 'gen_loss: ', 1.7702706145428238e-06)\n",
      "('recon_loss:', 0.0019939453341066837, 'disc_loss:', 7.742941670585424e-05, 'gen_loss: ', 3.4283175409655087e-06)\n",
      "('recon_loss:', 0.0025514145381748676, 'disc_loss:', 0.00010805930651258677, 'gen_loss: ', 4.295583948987769e-06)\n",
      "('recon_loss:', 0.0022164061665534973, 'disc_loss:', 5.50944751012139e-05, 'gen_loss: ', 1.7573599961906439e-06)\n"
     ]
    }
   ],
   "source": [
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_path = os.getcwd() + '/model_enc_' + str(fname)\n",
    "torch.save(Q.state_dict(),encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.45639705658\n"
     ]
    }
   ],
   "source": [
    "#entire_batch,batch_labels = get_train_batch(no_examples)\n",
    "tic = time.time()\n",
    "## It takes too much memory. Split in chunks and \n",
    "z_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "toc = time.time()\n",
    "print(toc-tic)\n",
    "z_encoded = z_encoded.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu,std = np.mean(z_encoded,axis=0),np.std(z_encoded,axis=0)\n",
    "# print mu.shape\n",
    "# z_encoded = (z_encoded - mu)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3467\n"
     ]
    }
   ],
   "source": [
    "#z_encoded,labels2 = sdc.clean_data(z_encoded,labels2,k=8)\n",
    "no_examples,ip_dim = z_encoded.shape\n",
    "z_encoded = z_encoded.astype(float)\n",
    "print no_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#z_encoded = (z_encoded + mu)*std\n",
    "labels2 = labels2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_encoded = z_encoded.cpu().data.numpy()[:,0]\n",
    "# y_encoded = z_encoded.cpu().data.numpy()[:,1]\n",
    "# w_encoded = z_encoded.cpu().data.numpy()[:,2]\n",
    "\n",
    "# # batch_labels_np = batch_labels_np.astype(int)\n",
    "# # print(batch_labels_np.dtype)\n",
    "# # print(batch_labels_np.shape)\n",
    "# batch_labels_np = list(labels2)\n",
    "\n",
    "# colors = []\n",
    "# for l in batch_labels_np:\n",
    "#     colors.append(\"C\"+str(int(l)))\n",
    "    \n",
    "# #plt.scatter(x_encoded,y_encoded,c=colors)\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(x_encoded,y_encoded,w_encoded,c=colors)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(size):\n",
    "    if generate_new_z == True:\n",
    "        print(\"gng here\")\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples+n_samples)[:size].numpy())\n",
    "        return new_z_encoded[ind], new_labels[ind]\n",
    "    else:\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples)[:size].numpy())\n",
    "        return z_encoded[ind], Variable(torch.cuda.LongTensor(labels2)[ind],requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,500)\n",
    "        self.l2 = nn.Linear(500,500)\n",
    "        self.l3 = nn.Linear(500,100)\n",
    "        #self.l4 = nn.Linear(400,70)\n",
    "        self.l5 = nn.Linear(100,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.l1(x))\n",
    "        x = F.selu(self.l2(x))\n",
    "        x = F.selu(self.l3(x))\n",
    "        #x = F.relu(self.l4(x))\n",
    "        x = (self.l5(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def gen_disc_model(w):\n",
    "#     d = Discriminator().cuda()\n",
    "#     d_optim = optim.Adam(d.parameters(),lr=1e-4)\n",
    "#     d = train_disc(d,d_optim,w)\n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def train_disc(d,d_optim,w):\n",
    "#     for ep in range(2000):\n",
    "#         d_optim.zero_grad()\n",
    "#         x,true_l = sample_z(200)\n",
    "#         true_l = true_l.view(true_l.size()[0],)\n",
    "#         p_labels = d(x)\n",
    "#         weights = torch.Tensor([1,w]).cuda()\n",
    "#         criteria = nn.CrossEntropyLoss(weight=weights)\n",
    "#         true_l = true_l.type(torch.cuda.LongTensor)\n",
    "#         loss = criteria(p_labels,true_l)\n",
    "#         loss.backward(retain_graph=True)\n",
    "#         d_optim.step()\n",
    "        \n",
    "# #         if(ep%50==49):\n",
    "# #             print(loss.data[0])\n",
    "            \n",
    "#     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch_z(batch_size,validation_iter=0,binary=True):\n",
    "    \n",
    "    if validation_iter == 0: #no validation\n",
    "        curr_data_size = no_examples\n",
    "        labels_train = labels2\n",
    "    else:\n",
    "        curr_data_size = int(no_examples*0.8)\n",
    "        interval_size = int(no_examples*0.2)\n",
    "        \n",
    "        if(val_iter==1):\n",
    "            s_ind1 = int((validation_iter)*interval_size)\n",
    "            end_ind1 = int((validation_iter+1)*interval_size)\n",
    "            s_ind2 = int((validation_iter + 1) * interval_size)\n",
    "            end_ind2 = int(no_examples)\n",
    "        else:\n",
    "            s_ind1 = 0\n",
    "            end_ind1 = int((validation_iter)*interval_size)\n",
    "            s_ind2 = int((validation_iter + 1) * interval_size)\n",
    "            end_ind2 = int(no_examples)\n",
    "        \n",
    "        #print(\"train_ind \",s_ind1,end_ind1,s_ind2,end_ind2)\n",
    "        indices = range(s_ind1,end_ind1) + range(s_ind2,end_ind2)\n",
    "        p_train_data = z_encoded[indices]\n",
    "        labels_train = labels2[indices]\n",
    "                               \n",
    "    samples = np.random.randint(low=0,high=curr_data_size,size=(batch_size,1))\n",
    "    if binary == True:\n",
    "        train_batch = p_train_data[samples].reshape(batch_size,ip_dim)\n",
    "        train_batch = train_batch.astype(float)\n",
    "    else:\n",
    "        None\n",
    "    \n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    target = Variable(torch.cuda.LongTensor(labels_train[samples]),requires_grad=False)\n",
    "    target = target.view(batch_size,)\n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_val_data_z(validation_iter,binary = True):\n",
    "    interval_size = int(no_examples)*0.2\n",
    "    s_ind = int((validation_iter-1)*interval_size)\n",
    "    e_ind = int((validation_iter) * interval_size)\n",
    "    if(binary==True):\n",
    "        train_data = z_encoded[s_ind:e_ind]\n",
    "    else:\n",
    "        None\n",
    "    labels_val = labels2[s_ind:e_ind]   \n",
    "    #print(\"val ind \",s_ind,e_ind)\n",
    "    return Variable(torch.cuda.FloatTensor(train_data)),labels_val  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('val iter: ', 1)\n",
      "('tn, fp, fn, tp: ', array([607,  38,   2,  46]))\n",
      "('saving model on val: ', 1, ' and weight: ', 15.0)\n",
      "('tn, fp, fn, tp: ', array([628,  17,   6,  42]))\n",
      "('tn, fp, fn, tp: ', array([579,  66,   0,  48]))\n",
      "('val iter: ', 2)\n",
      "('tn, fp, fn, tp: ', array([665,  28,   0,   0]))\n",
      "('saving model on val: ', 2, ' and weight: ', 15.0)\n",
      "('tn, fp, fn, tp: ', array([643,  50,   0,   0]))\n",
      "('tn, fp, fn, tp: ', array([642,  51,   0,   0]))\n",
      "('val iter: ', 3)\n",
      "('tn, fp, fn, tp: ', array([674,  20,   0,   0]))\n",
      "('saving model on val: ', 3, ' and weight: ', 15.0)\n",
      "('tn, fp, fn, tp: ', array([659,  35,   0,   0]))\n",
      "('tn, fp, fn, tp: ', array([662,  32,   0,   0]))\n",
      "('val iter: ', 4)\n",
      "('tn, fp, fn, tp: ', array([676,  17,   0,   0]))\n",
      "('saving model on val: ', 4, ' and weight: ', 15.0)\n",
      "('tn, fp, fn, tp: ', array([667,  26,   0,   0]))\n",
      "('tn, fp, fn, tp: ', array([673,  20,   0,   0]))\n",
      "('val iter: ', 5)\n",
      "('tn, fp, fn, tp: ', array([555,  34,  48,  57]))\n",
      "('tn, fp, fn, tp: ', array([555,  34,  43,  62]))\n",
      "('tn, fp, fn, tp: ', array([544,  45,  39,  66]))\n"
     ]
    }
   ],
   "source": [
    "min_fn = 15\n",
    "max_fp = 160\n",
    "cm_list = []\n",
    "batch_size = 128\n",
    "for i in range(1,6):\n",
    "    val_iter = i\n",
    "    print(\"val iter: \",val_iter)\n",
    "    \n",
    "   \n",
    "    #weights_array = [7]\n",
    "    weights = np.linspace(15,30,3)\n",
    "    for i,w in enumerate(weights): \n",
    "        mydisc = Discriminator().cuda()\n",
    "        optimizer = torch.optim.Adagrad(mydisc.parameters(),lr=1e-3)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor([1,w]))\n",
    "\n",
    "        for ep in range(3000):\n",
    "            train_batch,target = get_train_batch_z(batch_size,binary = True,validation_iter = val_iter)\n",
    "            model_op = mydisc(train_batch)\n",
    "            #print(model_op.type)\n",
    "            #print(target.type)\n",
    "            loss = criterion(model_op,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    #     if(ep%30==29):\n",
    "    #         print(loss.data[0])\n",
    "\n",
    "        ## After training check on cross validation data\n",
    "        val_data,labels_val = get_val_data_z(val_iter,binary = True)\n",
    "        train_op = mydisc(val_data)\n",
    "        train_op = train_op.cpu().data.numpy()\n",
    "        pred_labels = np.argmax(train_op,axis=1)\n",
    "        #tmp_labels = tmp_labels.data.cpu().numpy()\n",
    "        #print(sum(tmp_labels))\n",
    "        cf = metrics.confusion_matrix(labels_val,pred_labels).ravel()\n",
    "        #print(val_iter,w)\n",
    "        print('tn, fp, fn, tp: ',cf)\n",
    "        [tn,fp,fn,tp] = cf\n",
    "        wcf = [val_iter] + [w] + [cf]\n",
    "        if(fn < 3):\n",
    "            cm_list.append(wcf)\n",
    "            if(fn<4):\n",
    "                if(fp < max_fp):\n",
    "                    max_fp = fp\n",
    "                    model_path = os.getcwd() + '/disc_for_aae' + fname\n",
    "                    torch.save(mydisc.state_dict(),model_path)\n",
    "                    print(\"saving model on val: \",val_iter,\" and weight: \",w)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694, 1) (694,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "[544  45  39  66]\n"
     ]
    }
   ],
   "source": [
    "print labels_val.shape, pred_labels.shape\n",
    "print pred_labels[:10]\n",
    "#pred_labels = np.reshape(pred_labels.shape[0],1)\n",
    "cf = metrics.confusion_matrix(labels_val,pred_labels).ravel()\n",
    "print cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = os.getcwd() + '/model_autoencoder_' + str(fname)\n",
    "# for w in weights:\n",
    "#     print(\"w: \",w)\n",
    "#     d = gen_disc_model(w)\n",
    "#     train_op = d(train_encoded).cpu().data.numpy()\n",
    "#     train_op = np.argmax(train_op,axis=1)\n",
    "#     cf = metrics.confusion_matrix(labels_final,train_op)\n",
    "#     [tn, fp, fn, tp]  = cf.ravel()\n",
    "#     print('tn, fp, fn, tp: ',cf.ravel())\n",
    "#     if(fn < fn_min):\n",
    "#         fn_min = fn\n",
    "#         torch.save(d.state_dict(),model_path)\n",
    "#         print(\"saving model on weight: \",w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Testing Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fingerprints_test = []\n",
    "c_fingerprints_test = []\n",
    "labels_test = []\n",
    "with open(path+fname+'red_test.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints_test.append(row[:bf])\n",
    "        c_fingerprints_test.append(row[bf:-1])\n",
    "        labels_test.append(row[-1])\n",
    "        \n",
    "p_fingerprints_test = np.asarray(p_fingerprints_test)[1:]\n",
    "p_fingerprints_test = p_fingerprints_test.astype(int)\n",
    "p_fingerprints_test[(p_fingerprints_test==0)] = -1\n",
    "\n",
    "c_fingerprints_test = np.asarray(c_fingerprints_test)[1:]\n",
    "c_fingerprints_test = c_fingerprints_test.astype(float)\n",
    "\n",
    "#Normalise the features\n",
    "c_fingerprints_test = (c_fingerprints_test - np.mean(c_fingerprints_test,axis=0))/np.std(c_fingerprints_test,axis=0)\n",
    "\n",
    "fingerprints_test = np.concatenate((p_fingerprints_test,c_fingerprints_test),axis=1)\n",
    "\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples_test , ip_dim_test) = fingerprints_test.shape\n",
    "labels_test = labels_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tot_positive', 12)\n"
     ]
    }
   ],
   "source": [
    "labels2_test = np.zeros((len(labels_test),1))\n",
    "for i,l in enumerate(labels_test):\n",
    "    if l=='Active':\n",
    "        labels2_test[i] = 1\n",
    "    else:\n",
    "        labels2_test[i] = 0\n",
    "labels2_test = labels2_test.astype(int)\n",
    "total_pos = np.sum(labels2_test)\n",
    "print(\"tot_positive\",total_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fingerprints_test = Variable(torch.cuda.FloatTensor(p_fingerprints_test))\n",
    "z_test = Q(p_fingerprints_test)\n",
    "test_op = mydisc(z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tn, fp, fn, tp: ', array([840,   4,  12,   0]))\n"
     ]
    }
   ],
   "source": [
    "test_op = test_op.cpu().data.numpy()\n",
    "pred_labels = np.argmax(test_op,axis=1)\n",
    "cf = metrics.confusion_matrix(labels2_test,pred_labels).ravel()\n",
    "print('tn, fp, fn, tp: ',cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
