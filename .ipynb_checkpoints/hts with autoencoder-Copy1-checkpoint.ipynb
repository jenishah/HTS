{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import time\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('screen_info.txt','rb') as fl:\n",
    "    t = pickle.load(fl)\n",
    "fnames = t[0]\n",
    "totf = t[1]\n",
    "binf = t[2]\n",
    "runfile = 2\n",
    "fname = fnames[runfile]\n",
    "bf = binf[runfile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/bioassay-datasets/'\n",
    "p_fingerprints = []\n",
    "c_fingerprints = []\n",
    "labels = []\n",
    "with open(path+fname+'red_train.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints.append(row[:bf])\n",
    "        c_fingerprints.append(row[bf:-1])\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7986, 121)\n",
      "('total no of 1s', 42893)\n",
      "('total no of 0s', 923413)\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "p_fingerprints = p_fingerprints.astype(int)\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples , ip_dim) = p_fingerprints.shape\n",
    "labels = labels[1:]\n",
    "print(no_examples,ip_dim)\n",
    "print(\"total no of 1s\",np.sum(p_fingerprints))\n",
    "print(\"total no of 0s\",no_examples*ip_dim-np.sum(p_fingerprints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fingerprints[(p_fingerprints==0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = np.zeros((len(labels),1))\n",
    "for i,l in enumerate(labels):\n",
    "    if l=='Active':\n",
    "        labels2[i] = 1\n",
    "    else:\n",
    "        labels2[i] = 0\n",
    "labels2 = labels2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22]\n"
     ]
    }
   ],
   "source": [
    "no_active_ele = (sum(labels2))\n",
    "print(no_active_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = ip_dim\n",
    "h1_dim = 500\n",
    "h2_dim = 500\n",
    "h3_dim = 500\n",
    "z_dim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size):\n",
    "    samples = np.random.randint(low=0,high=no_examples,size=(batch_size,1))\n",
    "    train_batch = p_fingerprints[samples].reshape(batch_size,ip_dim)\n",
    "    train_batch = train_batch.astype(int)\n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    target = Variable(torch.cuda.FloatTensor(labels2[samples]),requires_grad=False)\n",
    "    \n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(X_dim,h1_dim)\n",
    "        self.l2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h3_dim)\n",
    "        self.l4 = nn.Linear(h3_dim,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,h3_dim)\n",
    "        self.l2 = nn.Linear(h3_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h1_dim)\n",
    "        self.l4 = nn.Linear(h1_dim,X_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.tanh(self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim+2,500)\n",
    "        self.lin2 = nn.Linear(500,100)\n",
    "        self.lin3 = nn.Linear(100,100)\n",
    "        self.lin4 = nn.Linear(100,30)\n",
    "        self.lin5 = nn.Linear(30,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.lin1(x))\n",
    "        x = F.selu(self.lin2(x))\n",
    "        x = F.selu(self.lin3(x))\n",
    "        x = F.selu(self.lin4(x))\n",
    "        x = F.sigmoid(self.lin5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label_info(y,batch_size):\n",
    "\n",
    "    tmp = np.zeros((batch_size,2))\n",
    "    tmp2 = np.zeros((batch_size,1))\n",
    "    y = y.cpu().data.numpy().reshape(batch_size,1)\n",
    "    tmp2[y==0] = 5\n",
    "    tmp3 = np.zeros((batch_size,1))\n",
    "    tmp3[y==1] = 5\n",
    "    tmp = np.concatenate((tmp2,tmp3),1)\n",
    "    label_info = torch.from_numpy((tmp)).cuda()\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size):\n",
    "    \n",
    "    for it in range(3500):\n",
    "        x,y = get_train_batch(batch_size)\n",
    "        z = Q(x)\n",
    "\n",
    "        #Reconstruction\n",
    "        \n",
    "        x_recon = P(z)\n",
    "        '''\n",
    "        x_recon[x_recon<0] = 0\n",
    "        x_recon[x_recon>0] = 1\n",
    "        x_tar = Variable(torch.cuda.FloatTensor(x.size()),requires_grad=False)\n",
    "        x_tar[x==-1] = 0\n",
    "        x_tar[x==1] = 1'''\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        CEL = criterion(x_recon, x)\n",
    "        \n",
    "        CEL.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        P_solver.step()\n",
    "        \n",
    "        Q.zero_grad()\n",
    "        P.zero_grad()\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        label_info = (add_label_info(y,batch_size))\n",
    "        z_false = np.concatenate((z.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "        z_false = Variable(torch.FloatTensor(z_false)).cuda()\n",
    "        #z_false = torch.cat((z,label_info),1)\n",
    "        z_true = np.random.rand(batch_size,z_dim)\n",
    "        z_true = np.concatenate((z_true,label_info.cpu().numpy()),1)\n",
    "        z_true = Variable(torch.FloatTensor(z_true).cuda())\n",
    "        #z_true = torch.cat((z_true,label_info),1)\n",
    "        z_true_op = Variable(D(z_true).data,requires_grad=False)\n",
    "        \n",
    "        z_false_op = D(z_false)\n",
    "        add_small = 1e-20\n",
    "        \n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss_d = criterion(z_false_op,z_true_op)\n",
    "        #loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "        loss_d.backward(retain_graph=True)\n",
    "        D_solver.step()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(it%50==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', CEL.data[0],'disc_loss:', loss_d.data[0],'gen_loss: ',G_loss.data[0])\n",
    "            #print(x_recon[0][:50].cpu().data.numpy().T)\n",
    "            #print()\n",
    "            #print(x[0][:50].cpu().data.numpy().T)\n",
    "           # print()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    Q = encoder()\n",
    "    Q.cuda()\n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-4)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-5)\n",
    "    P = decoder()\n",
    "    P.cuda()\n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-4)\n",
    "    D = disc()\n",
    "    D.cuda()\n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-3)\n",
    "    batch_size = 120\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size)\n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 0.9992547035217285, 'disc_loss:', 0.6928920745849609, 'gen_loss: ', 0.67038494348526)\n",
      "('recon_loss:', 0.16585776209831238, 'disc_loss:', 0.00829379539936781, 'gen_loss: ', 3.362654024385847e-05)\n",
      "('recon_loss:', 0.1497836858034134, 'disc_loss:', 0.040067605674266815, 'gen_loss: ', 0.002255338476970792)\n",
      "('recon_loss:', 0.15118305385112762, 'disc_loss:', 0.015966640785336494, 'gen_loss: ', 0.00035635617678053677)\n",
      "('recon_loss:', 0.1404351443052292, 'disc_loss:', 0.026898503303527832, 'gen_loss: ', 0.0032471977174282074)\n",
      "('recon_loss:', 0.13914090394973755, 'disc_loss:', 0.00925043411552906, 'gen_loss: ', 0.00039606387144885957)\n",
      "('recon_loss:', 0.14799293875694275, 'disc_loss:', 0.0125607093796134, 'gen_loss: ', 0.005583351012319326)\n",
      "('recon_loss:', 0.12539042532444, 'disc_loss:', 0.008006248623132706, 'gen_loss: ', 0.0005909120081923902)\n",
      "('recon_loss:', 0.13333380222320557, 'disc_loss:', 0.007734369020909071, 'gen_loss: ', 0.00038184135337360203)\n",
      "('recon_loss:', 0.11660851538181305, 'disc_loss:', 0.0068978904746472836, 'gen_loss: ', 0.0003743309644050896)\n",
      "('recon_loss:', 0.11477070301771164, 'disc_loss:', 0.007699699141085148, 'gen_loss: ', 0.0004831360129173845)\n",
      "('recon_loss:', 0.08396897464990616, 'disc_loss:', 0.013004799373447895, 'gen_loss: ', 0.005827236454933882)\n",
      "('recon_loss:', 0.08546764403581619, 'disc_loss:', 0.011902705766260624, 'gen_loss: ', 0.006396199110895395)\n",
      "('recon_loss:', 0.08333351463079453, 'disc_loss:', 0.005466571543365717, 'gen_loss: ', 0.00031241815304383636)\n",
      "('recon_loss:', 0.07873780280351639, 'disc_loss:', 0.004796213936060667, 'gen_loss: ', 0.000297853141091764)\n",
      "('recon_loss:', 0.07848212122917175, 'disc_loss:', 0.003959846217185259, 'gen_loss: ', 0.0002855498460121453)\n",
      "('recon_loss:', 0.06100626289844513, 'disc_loss:', 0.003318116534501314, 'gen_loss: ', 0.00015379850810859352)\n",
      "('recon_loss:', 0.06605251133441925, 'disc_loss:', 0.012510403990745544, 'gen_loss: ', 0.003272375324741006)\n",
      "('recon_loss:', 0.05074319243431091, 'disc_loss:', 0.002865489572286606, 'gen_loss: ', 0.00016855334979481995)\n",
      "('recon_loss:', 0.049914080649614334, 'disc_loss:', 0.0026306998915970325, 'gen_loss: ', 0.00020695595594588667)\n",
      "('recon_loss:', 0.04061511158943176, 'disc_loss:', 0.0025950109120458364, 'gen_loss: ', 0.0001366734941257164)\n",
      "('recon_loss:', 0.04614738002419472, 'disc_loss:', 0.0024336115457117558, 'gen_loss: ', 0.00018453801749274135)\n",
      "('recon_loss:', 0.046782687306404114, 'disc_loss:', 0.0023606945760548115, 'gen_loss: ', 7.900972559582442e-05)\n",
      "('recon_loss:', 0.050952326506376266, 'disc_loss:', 0.001746349036693573, 'gen_loss: ', 7.152204489102587e-05)\n",
      "('recon_loss:', 0.03685109317302704, 'disc_loss:', 0.11633358150720596, 'gen_loss: ', 2.0285981463530334e-06)\n",
      "('recon_loss:', 0.043959252536296844, 'disc_loss:', 0.007666274439543486, 'gen_loss: ', 0.00048637218424119055)\n",
      "('recon_loss:', 0.04474126547574997, 'disc_loss:', 0.04303336143493652, 'gen_loss: ', 0.0013391339452937245)\n",
      "('recon_loss:', 0.03790412098169327, 'disc_loss:', 0.06443961709737778, 'gen_loss: ', 0.015679100528359413)\n",
      "('recon_loss:', 0.042320095002651215, 'disc_loss:', 0.023074381053447723, 'gen_loss: ', 0.0025303459260612726)\n",
      "('recon_loss:', 0.04335683956742287, 'disc_loss:', 0.011658295057713985, 'gen_loss: ', 0.0008920843829400837)\n",
      "('recon_loss:', 0.04304176941514015, 'disc_loss:', 0.00911986269056797, 'gen_loss: ', 0.0005985276657156646)\n",
      "('recon_loss:', 0.04015346243977547, 'disc_loss:', 0.008928099647164345, 'gen_loss: ', 0.0005783449159935117)\n",
      "('recon_loss:', 0.03635082766413689, 'disc_loss:', 0.008436079137027264, 'gen_loss: ', 0.000517063366714865)\n",
      "('recon_loss:', 0.039063192903995514, 'disc_loss:', 0.020066993311047554, 'gen_loss: ', 0.011565028689801693)\n",
      "('recon_loss:', 0.039091017097234726, 'disc_loss:', 0.013413865119218826, 'gen_loss: ', 0.006088452413678169)\n",
      "('recon_loss:', 0.03521263971924782, 'disc_loss:', 0.00737899960950017, 'gen_loss: ', 0.00045642832992598414)\n",
      "('recon_loss:', 0.02987031079828739, 'disc_loss:', 0.01290323305875063, 'gen_loss: ', 0.006377020850777626)\n",
      "('recon_loss:', 0.03659750893712044, 'disc_loss:', 0.007135308347642422, 'gen_loss: ', 0.00044447064283303916)\n",
      "('recon_loss:', 0.037930503487586975, 'disc_loss:', 0.006819773931056261, 'gen_loss: ', 0.00042362321983091533)\n",
      "('recon_loss:', 0.03580232709646225, 'disc_loss:', 0.006710823159664869, 'gen_loss: ', 0.00039930050843395293)\n",
      "('recon_loss:', 0.03516276925802231, 'disc_loss:', 0.013034716248512268, 'gen_loss: ', 0.005428988020867109)\n",
      "('recon_loss:', 0.03970915451645851, 'disc_loss:', 0.006330120377242565, 'gen_loss: ', 0.0003740716492757201)\n",
      "('recon_loss:', 0.022309619933366776, 'disc_loss:', 0.005857738666236401, 'gen_loss: ', 0.00034737950772978365)\n",
      "('recon_loss:', 0.035995278507471085, 'disc_loss:', 0.005528244189918041, 'gen_loss: ', 0.00031290933839045465)\n",
      "('recon_loss:', 0.03205187991261482, 'disc_loss:', 0.0055594732984900475, 'gen_loss: ', 0.00032597576500847936)\n",
      "('recon_loss:', 0.031172096729278564, 'disc_loss:', 0.005336104426532984, 'gen_loss: ', 0.0003383103176020086)\n",
      "('recon_loss:', 0.03466371074318886, 'disc_loss:', 0.004829776473343372, 'gen_loss: ', 0.0002788785204757005)\n",
      "('recon_loss:', 0.03241657465696335, 'disc_loss:', 0.011728418059647083, 'gen_loss: ', 0.005055292509496212)\n",
      "('recon_loss:', 0.03347979485988617, 'disc_loss:', 0.0048933313228189945, 'gen_loss: ', 0.0002975839306600392)\n",
      "('recon_loss:', 0.030875321477651596, 'disc_loss:', 0.010739701800048351, 'gen_loss: ', 0.005750827956944704)\n",
      "('recon_loss:', 0.027259984984993935, 'disc_loss:', 0.004723727703094482, 'gen_loss: ', 0.0002676869626156986)\n",
      "('recon_loss:', 0.035061608999967575, 'disc_loss:', 0.004702463746070862, 'gen_loss: ', 0.00024382217088714242)\n",
      "('recon_loss:', 0.029392121359705925, 'disc_loss:', 0.011731740087270737, 'gen_loss: ', 0.004826937336474657)\n",
      "('recon_loss:', 0.028099490329623222, 'disc_loss:', 0.004532395862042904, 'gen_loss: ', 0.00027322189998812973)\n",
      "('recon_loss:', 0.02785453014075756, 'disc_loss:', 0.004455797374248505, 'gen_loss: ', 0.00024848233442753553)\n",
      "('recon_loss:', 0.028245694935321808, 'disc_loss:', 0.004467991180717945, 'gen_loss: ', 0.000258572370512411)\n",
      "('recon_loss:', 0.022708455100655556, 'disc_loss:', 0.01002452615648508, 'gen_loss: ', 0.006001924630254507)\n",
      "('recon_loss:', 0.019548123702406883, 'disc_loss:', 0.004158220253884792, 'gen_loss: ', 0.00023579684784635901)\n",
      "('recon_loss:', 0.02502986043691635, 'disc_loss:', 0.004194500856101513, 'gen_loss: ', 0.0002327606634935364)\n",
      "('recon_loss:', 0.021712355315685272, 'disc_loss:', 0.003924405202269554, 'gen_loss: ', 0.00021956571436021477)\n",
      "('recon_loss:', 0.02614980936050415, 'disc_loss:', 0.010488655418157578, 'gen_loss: ', 0.0051358407363295555)\n",
      "('recon_loss:', 0.021211136132478714, 'disc_loss:', 0.00375942001119256, 'gen_loss: ', 0.00025836442364379764)\n",
      "('recon_loss:', 0.0272454172372818, 'disc_loss:', 0.0031779122073203325, 'gen_loss: ', 0.000162377895321697)\n",
      "('recon_loss:', 0.026278918609023094, 'disc_loss:', 0.003515340853482485, 'gen_loss: ', 0.00018193673167843372)\n",
      "('recon_loss:', 0.02625448815524578, 'disc_loss:', 0.0023278172593563795, 'gen_loss: ', 0.00012490020890254527)\n",
      "('recon_loss:', 0.02006475068628788, 'disc_loss:', 0.0023082199040800333, 'gen_loss: ', 0.00012519341544248164)\n",
      "('recon_loss:', 0.027744118124246597, 'disc_loss:', 0.0022872467525303364, 'gen_loss: ', 0.00010934342571999878)\n",
      "('recon_loss:', 0.025245895609259605, 'disc_loss:', 0.0024153694976121187, 'gen_loss: ', 0.00013473709987010807)\n",
      "('recon_loss:', 0.02565293200314045, 'disc_loss:', 0.00236888718791306, 'gen_loss: ', 0.00012079477892257273)\n",
      "('recon_loss:', 0.027637982740998268, 'disc_loss:', 0.0024626986123621464, 'gen_loss: ', 0.00013085232058074325)\n"
     ]
    }
   ],
   "source": [
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_path = os.getcwd() + '/model_enc_' + str(fname)\n",
    "torch.save(Q.state_dict(),encoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.41320896149\n"
     ]
    }
   ],
   "source": [
    "#entire_batch,batch_labels = get_train_batch(no_examples)\n",
    "tic = time.time()\n",
    "## It takes too much memory. Split in chunks and \n",
    "z_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "toc = time.time()\n",
    "print(toc-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_new_z = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate new samples from orignal ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if generate_new_z == True:\n",
    "    n_comb = 5\n",
    "    n_samples = 100\n",
    "    extra_samples = torch.cuda.FloatTensor(n_samples,z_dim)\n",
    "    extra_labels = Variable(torch.ones(n_samples).cuda())\n",
    "    for i in range(n_samples):\n",
    "        #coeff = np.random.rand(n_comb,1)\n",
    "        coeff = Variable(torch.randn(n_comb,1)).cuda()\n",
    "        active_z_encoded = z_encoded[torch.cuda.FloatTensor(labels2)==1]\n",
    "        tmp_rand_nos = torch.randperm(int(no_active_ele))\n",
    "        rand_nos = tmp_rand_nos[0:n_comb].cuda()\n",
    "        rand_z = torch.transpose(z_encoded[rand_nos],0,1)\n",
    "        extra_samples[i] = torch.cuda.FloatTensor(torch.matmul(rand_z,coeff).data)\n",
    "    extra_samples = Variable(extra_samples)\n",
    "    \n",
    "    new_z_encoded = torch.cat((z_encoded,extra_samples),0)\n",
    "    new_labels = torch.cat((Variable(torch.cuda.FloatTensor(labels2)),extra_labels),0)\n",
    "    perm = torch.randperm(no_examples+n_samples).cuda()\n",
    "    new_z_encoded = new_z_encoded[perm]\n",
    "    new_labels = new_labels[perm]\n",
    "    batch_labels_np = new_labels.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x_encoded = z_encoded.cpu().data.numpy()[:,0]\n",
    "# y_encoded = z_encoded.cpu().data.numpy()[:,1]\n",
    "# w_encoded = z_encoded.cpu().data.numpy()[:,2]\n",
    "\n",
    "# # batch_labels_np = batch_labels_np.astype(int)\n",
    "# # print(batch_labels_np.dtype)\n",
    "# # print(batch_labels_np.shape)\n",
    "# batch_labels_np = list(labels2)\n",
    "\n",
    "# colors = []\n",
    "# for l in batch_labels_np:\n",
    "#     colors.append(\"C\"+str(int(l)))\n",
    "    \n",
    "# #plt.scatter(x_encoded,y_encoded,c=colors)\n",
    "# fig = plt.figure()\n",
    "# ax = Axes3D(fig)\n",
    "# ax.scatter(x_encoded,y_encoded,w_encoded,c=colors)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(size):\n",
    "    if generate_new_z == True:\n",
    "        print(\"gng here\")\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples+n_samples)[:size].numpy())\n",
    "        return new_z_encoded[ind], new_labels[ind]\n",
    "    else:\n",
    "        ind = torch.cuda.LongTensor(torch.randperm(no_examples)[:size].numpy())\n",
    "        return z_encoded[ind], Variable(torch.cuda.LongTensor(labels2)[ind],requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,800)\n",
    "        self.l2 = nn.Linear(800,500)\n",
    "        self.l3 = nn.Linear(500,400)\n",
    "        self.l4 = nn.Linear(400,70)\n",
    "        self.l5 = nn.Linear(70,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.selu(self.l1(x))\n",
    "        x = F.selu(self.l2(x))\n",
    "        x = F.selu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        x = (self.l5(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_disc_model(w):\n",
    "    d = Discriminator().cuda()\n",
    "    d_optim = optim.Adam(d.parameters(),lr=1e-4)\n",
    "    d = train_disc(d,d_optim,w)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_disc(d,d_optim,w):\n",
    "    for ep in range(2000):\n",
    "        d_optim.zero_grad()\n",
    "        x,true_l = sample_z(200)\n",
    "        true_l = true_l.view(true_l.size()[0],)\n",
    "        p_labels = d(x)\n",
    "        weights = torch.Tensor([1,w]).cuda()\n",
    "        criteria = nn.CrossEntropyLoss(weight=weights)\n",
    "        true_l = true_l.type(torch.cuda.LongTensor)\n",
    "        loss = criteria(p_labels,true_l)\n",
    "        loss.backward(retain_graph=True)\n",
    "        d_optim.step()\n",
    "        \n",
    "#         if(ep%50==49):\n",
    "#             print(loss.data[0])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = np.linspace(5,30,25)\n",
    "# with open(\"cnt_test_good)weights.txt\",'rb') as f:\n",
    "#     weights = pickle.load(f)\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if generate_new_z == True:\n",
    "#     train_encoded = (new_z_encoded)\n",
    "#     labels_final = batch_labels_np\n",
    "# else:\n",
    "train_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "labels_final = labels2\n",
    "fn_min  = 48\n",
    "    \n",
    "cm_autoencoder = []\n",
    "cm_autoencoder.append(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w: ', 5.0)\n",
      "('tn, fp, fn, tp: ', array([7955,    9,    9,   13]))\n",
      "('saving model on weight: ', 5.0)\n",
      "('w: ', 6.041666666666667)\n",
      "('tn, fp, fn, tp: ', array([7955,    9,   10,   12]))\n",
      "('w: ', 7.0833333333333339)\n",
      "('tn, fp, fn, tp: ', array([7943,   21,    8,   14]))\n",
      "('saving model on weight: ', 7.0833333333333339)\n",
      "('w: ', 8.125)\n"
     ]
    }
   ],
   "source": [
    "model_path = os.getcwd() + '/model_autoencoder_' + str(fname)\n",
    "for w in weights:\n",
    "    print(\"w: \",w)\n",
    "    d = gen_disc_model(w)\n",
    "    train_op = d(train_encoded).cpu().data.numpy()\n",
    "    train_op = np.argmax(train_op,axis=1)\n",
    "    cf = metrics.confusion_matrix(labels_final,train_op)\n",
    "    [tn, fp, fn, tp]  = cf.ravel()\n",
    "    print('tn, fp, fn, tp: ',cf.ravel())\n",
    "    if(fn < fn_min):\n",
    "        fn_min = fn\n",
    "        torch.save(d.state_dict(),model_path)\n",
    "        print(\"saving model on weight: \",w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"autoencoder_arti_ex_cm\",'wb') as f:\n",
    "    pickle.dump(cm_autoencoder,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"autoencoder_1.txt\",'wb') as fb:\n",
    "    pickle.dump(cm_autoencoder,fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
