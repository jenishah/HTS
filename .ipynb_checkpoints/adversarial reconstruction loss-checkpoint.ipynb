{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "from torch.autograd import Variable\n",
    "import sklearn.metrics as metrics\n",
    "import pickle\n",
    "import os\n",
    "import numpy.linalg as la\n",
    "import random\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor()])#,transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
    "train_set = dset.MNIST('/home/daiict/Desktop/udit/C-GAN./data' ,train=True, download= True,\n",
    "                       transform = transform)\n",
    "test_set = dset.MNIST('/home/daiict/Desktop/udit/C-GAN./data' ,train=False, download=True,\n",
    "                       transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000,shuffle=True)\n",
    "\n",
    "print len(train_loader), len(test_loader)\n",
    "\n",
    "ip_dim = 28*28\n",
    "z_dim = 2\n",
    "comb_dim = ip_dim + z_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xtmp,label_tmp = next(iter(test_loader))\n",
    "# xtmp = xtmp.numpy()\n",
    "# xtmp = xtmp.reshape(1000,28*28)\n",
    "# X_embedded = TSNE(n_components=2).fit_transform(xtmp)\n",
    "# plt.figure(figsize=(8, 6)) \n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1],c = labels2,cmap=plt.cm.hot)\n",
    "# plt.colorbar()\n",
    "# plt.grid()\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Encoder\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(ip_dim,500)\n",
    "        self.l2 = nn.Linear(500,500)\n",
    "        self.l3 = nn.Linear(500,300)\n",
    "        self.l4 = nn.Linear(300,300)\n",
    "        self.l5 = nn.Linear(300,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.leaky_relu(self.l4(x))\n",
    "        x = (self.l5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "### Decoder\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim+10,300)\n",
    "        self.l2 = nn.Linear(300,300)\n",
    "        self.l3 = nn.Linear(300,500)\n",
    "        self.l4 = nn.Linear(500,500)\n",
    "        self.l5 = nn.Linear(500,ip_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.leaky_relu(self.l4(x))\n",
    "        x = F.sigmoid(self.l5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "###  Discriminator\n",
    "\n",
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim+10,100)\n",
    "        self.l2 = nn.Linear(100,100)\n",
    "        self.l3 = nn.Linear(100,100)\n",
    "        self.l4 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.sigmoid(self.l4(x))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class disc2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc2,self).__init__()\n",
    "        self.l1 = nn.Linear(ip_dim,500)\n",
    "        self.l2 = nn.Linear(500,500)\n",
    "        self.l3 = nn.Linear(500,100)\n",
    "        self.l4 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.sigmoid(self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label_info(y,batch_size,numpy=False):\n",
    "\n",
    "    tmp = np.zeros((batch_size,10))\n",
    "    if(numpy == False):\n",
    "        y = y.cpu().numpy().reshape(batch_size,1)\n",
    "    for i in range(y.shape[0]):\n",
    "        tmp[i,y[i]] = 5\n",
    "    label_info = torch.from_numpy((tmp))\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_true_z(label):\n",
    "    \n",
    "    noise = np.random.randn(batch_size,z_dim)\n",
    "    label = label.numpy().reshape(batch_size,1)\n",
    "    #print noise.shape,label.shape\n",
    "    z = 0.5*noise + label*10\n",
    "    z = Variable(torch.FloatTensor(z).cuda())\n",
    "    return z\n",
    "    #z = torch.add(noise,value=1,other=label)\n",
    "    #z = Variable(z.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-cadd4c6ef900>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-cadd4c6ef900>\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    if(it%==0):\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,D2,D2_solver,batch_size):\n",
    "    recon = []\n",
    "    encode = []\n",
    "    discriminate = []\n",
    "    for it in range(2,5000):\n",
    "        x,y = next(iter(train_loader))\n",
    "       \n",
    "        x = x.view(batch_size,28*28)\n",
    "        if(cuda==True):\n",
    "            x = Variable(x.cuda())\n",
    "            label_info = Variable((add_label_info(y,batch_size)).type(torch.FloatTensor)).cuda()\n",
    "\n",
    "        else:\n",
    "            x = Variable(x)\n",
    "            label_info = Variable((add_label_info(y,batch_size)).type(torch.FloatTensor))\n",
    "\n",
    "        z = Q(x)\n",
    "\n",
    "        #Reconstruction\n",
    "\n",
    "        z_false = (torch.cat([z,label_info],1))\n",
    "        \n",
    "        x_recon = P(z_false)\n",
    "        \n",
    "        add_small = 1e-20\n",
    "        \n",
    "#         if(it%1==0):\n",
    "#             CEL = F.binary_cross_entropy(x_recon,x)\n",
    "#             #CEL = criterion(x_recon, x)\n",
    "        \n",
    "#             CEL.backward(retain_graph=True)\n",
    "#             Q_solver.step()\n",
    "#             P_solver.step()\n",
    "        \n",
    "#             Q.zero_grad()\n",
    "#             P.zero_grad()\n",
    "#         recon.append(CEL)\n",
    "\n",
    "## use a discriminator to learn the reconstruction loss also discriminatively\n",
    "        if(it%==0):\n",
    "            Dx_false = D2(x_recon)\n",
    "            Dx_true = D2(x)\n",
    "            \n",
    "            D2_loss = -torch.mean(torch.log(Dx_true + add_small) + torch.log(1 - Dx_false + add_small))\n",
    "            D2_loss.backward(retain_graph=True)\n",
    "            D2_solver.step()\n",
    "            D2.zero_grad()\n",
    "        \n",
    "            Dx_false = D2(x_recon)\n",
    "            \n",
    "            recon_loss = -torch.mean(torch.log(Dx_false))\n",
    "            recon_loss.backward(retain_graph=True)\n",
    "            Q_solver.step()\n",
    "            P_solver.step()\n",
    "            Q.zero_grad()\n",
    "            P.zero_grad()\n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #Discriminator\n",
    "       \n",
    "        z_true = Variable(torch.randn(batch_size,z_dim).cuda(),requires_grad = False)\n",
    "        if cuda==True:\n",
    "            z_true = Variable(torch.cat([z_true,label_info],1).data,requires_grad = False).cuda()\n",
    "        else:\n",
    "            z_true = Variable(torch.cat([z_true,label_info],1).data,requires_grad = False)\n",
    "\n",
    "        z_true_op = D(z_true)\n",
    "        \n",
    "        z_false_op = D(z_false)\n",
    "        \n",
    "        \n",
    "        if(it%1==0):\n",
    "    \n",
    "            loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "            loss_d.backward(retain_graph=True)\n",
    "            D_solver.step()\n",
    "            D.zero_grad()\n",
    "            discriminate.append(loss_d)\n",
    "\n",
    "    #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward(retain_graph=True)\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        encode.append(G_loss)\n",
    "        \n",
    "        \n",
    "    ## occasionally update the encoder with MSE loss\n",
    "    \n",
    "#         if(it%25==0):\n",
    "#             tmp_loss = nn.MSELoss()(z_false,z_true)\n",
    "#             print('tmp_loss:',tmp_loss.data[0])\n",
    "#             tmp_loss = 1e-3*tmp_loss\n",
    "#             tmp_loss.backward()\n",
    "#             Q_solver.step()\n",
    "#             Q_solver.zero_grad()\n",
    "\n",
    "        \n",
    "        \n",
    "        if(it%25==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', recon_loss.data[0],'disc_loss:', loss_d.data[0])\n",
    "            print('gen_loss: ',G_loss.data[0],'D2_loss',D2_loss.data[0])\n",
    "#             plt.plot(range(1,it),np.array(discriminate).reshape(len(discriminate),1))\n",
    "#             plt.title('discriminator loss')\n",
    "#             plot.show()\n",
    "            \n",
    "            \n",
    "#             plt.plot(range(1,it),np.array(encode).reshape(len(encode),1))\n",
    "#             plt.title('encoder loss')\n",
    "#             plot.show()\n",
    "           \n",
    "#             plt.plot(range(1,it),np.array(reconstruct).reshape(len(reconstruct),1))\n",
    "#             plt.title('reconstruction loss')\n",
    "#             plot.show()            \n",
    "        \n",
    "        ## plot the distribution ##\n",
    "        if(it%25 == 0):\n",
    "            xcheck,labels = next(iter(test_loader))\n",
    "            xcheck = xcheck.view(1000,28*28)\n",
    "            xcheck = Variable(xcheck.cuda())\n",
    "            labels_sc = labels.cpu().numpy()\n",
    "            #xcheck = Variable(xcheck.view(1000,28*28).cuda())\n",
    "            zhat = Q(xcheck)\n",
    "            z_mu = zhat.cpu().data.numpy()\n",
    "            plt.figure(figsize=(8, 6)) \n",
    "           \n",
    "            plt.scatter(z_mu[:, 0], z_mu[:, 1],c = labels_sc)#,cmap=plt.cm.autumn)\n",
    "            plt.colorbar()\n",
    "            plt.grid()\n",
    "            plt.show()   \n",
    "            \n",
    "            zcheck = Variable(torch.randn(1,z_dim).cuda())\n",
    "            \n",
    "            y = np.array([2])\n",
    "            label_info = (add_label_info(y,1,numpy=True))\n",
    "            z_false = np.concatenate((zcheck.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "            z_false = Variable(torch.FloatTensor(z_false)).cuda()\n",
    "            print z_false.size()\n",
    "            x_recon = P(z_false)\n",
    "            op = x_recon.cpu().data.numpy()\n",
    "            op = op.reshape(28,28)\n",
    "            plt.imshow(op)\n",
    "            plt.show()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    \n",
    "    if cuda==True:\n",
    "        Q = encoder().cuda()\n",
    "        P = decoder().cuda()\n",
    "        D = disc().cuda()\n",
    "        D2 = disc2().cuda()\n",
    "    else:\n",
    "        Q = encoder()\n",
    "        P = decoder()\n",
    "        D = disc()\n",
    "   \n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-3)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-3)\n",
    "    \n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-3)\n",
    "   \n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-3)\n",
    "    D2_solver = optim.Adam(D2.parameters(),lr = 1e-3)\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,D2,D2_solver,batch_size)\n",
    "    \n",
    " \n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True\n",
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zcheck = Variable(torch.randn(1,z_dim).cuda())\n",
    "y = np.array([6])\n",
    "label_info = (add_label_info(y,1,numpy=True))\n",
    "z_false = np.concatenate((zcheck.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "z_false = Variable(torch.FloatTensor(z_false)).cuda()\n",
    "print z_false.size()\n",
    "x_recon = P(z_false)\n",
    "op = x_recon.cpu().data.numpy()\n",
    "op = op.reshape(28,28)\n",
    "plt.imshow(op)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if(1==1):\n",
    "#     xcheck,labels = next(iter(test_loader))\n",
    "#     labels2 = labels.numpy()\n",
    "#     xcheck = Variable(xcheck.view(1000,28*28).cuda())\n",
    "#     zhat = E(xcheck)\n",
    "#     z_mu = zhat.cpu().data.numpy()\n",
    "#     print labels2.shape\n",
    "#     plt.figure(figsize=(8, 6)) \n",
    "#     #colors = cm.rainbow(np.linspace(0, 1, 10))\n",
    "#     plt.scatter(z_mu[:, 0], z_mu[:, 1],c = labels2,cmap=plt.cm.hot)\n",
    "#     plt.colorbar()\n",
    "#     plt.grid()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_embedded = TSNE(n_components=2).fit_transform(z_mu)\n",
    "# plt.figure(figsize=(8, 6)) \n",
    "# plt.scatter(X_embedded[:, 0], X_embedded[:, 1],c = labels2,cmap=plt.cm.hot)\n",
    "# plt.colorbar()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noise = np.random.randn(1,z_dim)\n",
    "# label = 7\n",
    "#     #print noise.shape,label.shape\n",
    "# z = 0.5*noise + label*10\n",
    "# z = Variable(torch.FloatTensor(z).cuda())\n",
    "# op = G(z).cpu().data.numpy()\n",
    "# op = op.reshape(28,28)\n",
    "# plt.imshow(op)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
