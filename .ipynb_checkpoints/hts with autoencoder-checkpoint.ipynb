{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import sklearn.metrics as metrics\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/daiict/CVShare/Jeni/hts/bioassay-datasets/'\n",
    "p_fingerprints = []\n",
    "labels = []\n",
    "with open(path+'AID362red_train.csv') as csvfile:\n",
    "    readcsv = csv.reader(csvfile)\n",
    "    for row in readcsv:\n",
    "        p_fingerprints.append(row[:112])\n",
    "        labels.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3423, 112)\n",
      "('total no of 1s', 25982)\n",
      "('total no of 0s', 357394)\n"
     ]
    }
   ],
   "source": [
    "p_fingerprints = np.asarray(p_fingerprints)[1:]\n",
    "p_fingerprints = p_fingerprints.astype(int)\n",
    "#p2_fingerprints = np.ones(p_fingerprints.shape)\n",
    "(no_examples , ip_dim) = p_fingerprints.shape\n",
    "labels = labels[1:]\n",
    "print(no_examples,ip_dim)\n",
    "print(\"total no of 1s\",np.sum(p_fingerprints))\n",
    "print(\"total no of 0s\",no_examples*ip_dim-np.sum(p_fingerprints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_fingerprints[(p_fingerprints==0)] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels2 = np.zeros((len(labels),1))\n",
    "for i,l in enumerate(labels):\n",
    "    if l=='Active':\n",
    "        labels2[i] = 1\n",
    "    else:\n",
    "        labels2[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 48.]\n"
     ]
    }
   ],
   "source": [
    "no_active_ele = (sum(labels2))\n",
    "print(no_active_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_dim = ip_dim\n",
    "h1_dim = 500\n",
    "h2_dim = 500\n",
    "h3_dim = 500\n",
    "z_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_batch(batch_size):\n",
    "    samples = np.random.randint(low=0,high=no_examples,size=(batch_size,1))\n",
    "    train_batch = p_fingerprints[samples].reshape(batch_size,ip_dim)\n",
    "    train_batch = train_batch.astype(int)\n",
    "    train_batch = torch.cuda.FloatTensor(train_batch)\n",
    "    train_batch = Variable(train_batch,requires_grad=False).cuda()\n",
    "    target = Variable(torch.cuda.FloatTensor(labels2[samples]),requires_grad=False)\n",
    "    \n",
    "    return train_batch,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(X_dim,h1_dim)\n",
    "        self.l2 = nn.Linear(h1_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h3_dim)\n",
    "        self.l4 = nn.Linear(h3_dim,z_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = self.l4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,h3_dim)\n",
    "        self.l2 = nn.Linear(h3_dim,h2_dim)\n",
    "        self.l3 = nn.Linear(h2_dim,h1_dim)\n",
    "        self.l4 = nn.Linear(h1_dim,X_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.leaky_relu(self.l3(x))\n",
    "        x = F.tanh(self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class disc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(disc,self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim+2,300)\n",
    "        self.lin2 = nn.Linear(300,100)\n",
    "        self.lin3 = nn.Linear(100,30)\n",
    "        self.lin4 = nn.Linear(30,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.leaky_relu(self.lin3(x))\n",
    "        x = F.sigmoid(self.lin4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_label_info(y,batch_size):\n",
    "\n",
    "    tmp = np.zeros((batch_size,2))\n",
    "    tmp2 = np.zeros((batch_size,1))\n",
    "    y = y.cpu().data.numpy().reshape(batch_size,1)\n",
    "    tmp2[y==0] = 5\n",
    "    tmp3 = np.zeros((batch_size,1))\n",
    "    tmp3[y==1] = 5\n",
    "    tmp = np.concatenate((tmp2,tmp3),1)\n",
    "    label_info = torch.from_numpy((tmp)).cuda()\n",
    "    return label_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size):\n",
    "    \n",
    "    for it in range(2000):\n",
    "        x,y = get_train_batch(batch_size)\n",
    "        z = Q(x)\n",
    "\n",
    "        #Reconstruction\n",
    "        \n",
    "        x_recon = P(z)\n",
    "        '''\n",
    "        x_recon[x_recon<0] = 0\n",
    "        x_recon[x_recon>0] = 1\n",
    "        x_tar = Variable(torch.cuda.FloatTensor(x.size()),requires_grad=False)\n",
    "        x_tar[x==-1] = 0\n",
    "        x_tar[x==1] = 1'''\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        CEL = criterion(x_recon, x)\n",
    "        \n",
    "        CEL.backward()\n",
    "        Q_solver.step()\n",
    "        P_solver.step()\n",
    "        \n",
    "        Q.zero_grad()\n",
    "        P.zero_grad()\n",
    "        \n",
    "        #Discriminator\n",
    "        \n",
    "        label_info = (add_label_info(y,batch_size))\n",
    "        z_false = np.concatenate((z.cpu().data.numpy(),label_info.cpu().numpy()),1)\n",
    "        z_false = Variable(torch.FloatTensor(z_false).cuda())\n",
    "        #z_false = torch.cat((z,label_info),1)\n",
    "        z_true = np.random.rand(batch_size,z_dim)\n",
    "        z_true = np.concatenate((z_true,label_info.cpu().numpy()),1)\n",
    "        z_true = Variable(torch.FloatTensor(z_true).cuda())\n",
    "        #z_true = torch.cat((z_true,label_info),1)\n",
    "        z_true_op = Variable(D(z_true).data,requires_grad=False)\n",
    "        \n",
    "        z_false_op = D(z_false)\n",
    "        add_small = 1e-20\n",
    "        \n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        loss_d = criterion(z_false_op,z_true_op)\n",
    "        #loss_d = -torch.mean(torch.log(z_true_op + add_small) + torch.log(1 - z_false_op + add_small))\n",
    "        loss_d.backward(retain_variables = True)\n",
    "        D_solver.step()\n",
    "        D.zero_grad()\n",
    "        \n",
    "        #Updating the encoder\n",
    "        \n",
    "        G_loss = -torch.mean(torch.log(z_false_op+1e-20))\n",
    "        G_loss.backward()\n",
    "        Q_solver.step()\n",
    "        Q_solver.zero_grad()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if(it%50==0):\n",
    "            #print(extra_loss.data[0],CEL.data[0])\n",
    "            print('recon_loss:', CEL.data[0],'disc_loss:', loss_d.data[0],'gen_loss: ',G_loss.data[0])\n",
    "            #print(x_recon[0][:50].cpu().data.numpy().T)\n",
    "            #print()\n",
    "            #print(x[0][:50].cpu().data.numpy().T)\n",
    "           # print()\n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    Q = encoder()\n",
    "    Q.cuda()\n",
    "    Q_solver = optim.Adam(Q.parameters(),lr=1e-4)\n",
    "    E_solver = optim.Adam(Q.parameters(),lr = 1e-5)\n",
    "    P = decoder()\n",
    "    P.cuda()\n",
    "    P_solver = optim.Adam(P.parameters(),lr = 1e-4)\n",
    "    D = disc()\n",
    "    D.cuda()\n",
    "    D_solver = optim.Adam(D.parameters(),lr = 1e-4)\n",
    "    batch_size = 120\n",
    "    Q,P = train_model(Q,Q_solver,P,P_solver,D,D_solver,batch_size)\n",
    "    \n",
    "    return Q,P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('recon_loss:', 0.9965453147888184, 'disc_loss:', 0.6874924898147583, 'gen_loss: ', 0.5845605731010437)\n",
      "('recon_loss:', 0.22615621984004974, 'disc_loss:', 0.7811915278434753, 'gen_loss: ', 0.2230936586856842)\n",
      "('recon_loss:', 0.20745374262332916, 'disc_loss:', 0.7680944800376892, 'gen_loss: ', 0.22499129176139832)\n",
      "('recon_loss:', 0.20160159468650818, 'disc_loss:', 0.7696407437324524, 'gen_loss: ', 0.2243676632642746)\n",
      "('recon_loss:', 0.17638222873210907, 'disc_loss:', 0.7617305517196655, 'gen_loss: ', 0.22113513946533203)\n",
      "('recon_loss:', 0.14872747659683228, 'disc_loss:', 0.745427131652832, 'gen_loss: ', 0.19750909507274628)\n",
      "('recon_loss:', 0.14413218200206757, 'disc_loss:', 0.7322324514389038, 'gen_loss: ', 0.21477489173412323)\n",
      "('recon_loss:', 0.12986476719379425, 'disc_loss:', 0.7394369840621948, 'gen_loss: ', 0.20232196152210236)\n",
      "('recon_loss:', 0.10781777650117874, 'disc_loss:', 0.741156816482544, 'gen_loss: ', 0.19699278473854065)\n",
      "('recon_loss:', 0.10728238523006439, 'disc_loss:', 0.7383509874343872, 'gen_loss: ', 0.1985492706298828)\n",
      "('recon_loss:', 0.11249645054340363, 'disc_loss:', 0.7370657324790955, 'gen_loss: ', 0.19627517461776733)\n",
      "('recon_loss:', 0.11706699430942535, 'disc_loss:', 0.7364152073860168, 'gen_loss: ', 0.18963973224163055)\n",
      "('recon_loss:', 0.09869435429573059, 'disc_loss:', 0.7339096665382385, 'gen_loss: ', 0.19094565510749817)\n",
      "('recon_loss:', 0.09806902706623077, 'disc_loss:', 0.7321082949638367, 'gen_loss: ', 0.18973778188228607)\n",
      "('recon_loss:', 0.08995973318815231, 'disc_loss:', 0.7437468767166138, 'gen_loss: ', 0.1774868667125702)\n",
      "('recon_loss:', 0.09103994071483612, 'disc_loss:', 0.7537843585014343, 'gen_loss: ', 0.16864435374736786)\n",
      "('recon_loss:', 0.08771563321352005, 'disc_loss:', 0.7228402495384216, 'gen_loss: ', 0.20279769599437714)\n",
      "('recon_loss:', 0.0801638811826706, 'disc_loss:', 0.7366546392440796, 'gen_loss: ', 0.18858696520328522)\n",
      "('recon_loss:', 0.07950712740421295, 'disc_loss:', 0.7348770499229431, 'gen_loss: ', 0.18800844252109528)\n",
      "('recon_loss:', 0.07620248198509216, 'disc_loss:', 0.7320970892906189, 'gen_loss: ', 0.19313783943653107)\n",
      "('recon_loss:', 0.07505505532026291, 'disc_loss:', 0.7344210743904114, 'gen_loss: ', 0.19034618139266968)\n",
      "('recon_loss:', 0.06694050878286362, 'disc_loss:', 0.7377422451972961, 'gen_loss: ', 0.18416520953178406)\n",
      "('recon_loss:', 0.06284001469612122, 'disc_loss:', 0.7269741892814636, 'gen_loss: ', 0.19789262115955353)\n",
      "('recon_loss:', 0.06909966468811035, 'disc_loss:', 0.7421152591705322, 'gen_loss: ', 0.1816624253988266)\n",
      "('recon_loss:', 0.07168486714363098, 'disc_loss:', 0.7257325649261475, 'gen_loss: ', 0.19867992401123047)\n",
      "('recon_loss:', 0.06611952185630798, 'disc_loss:', 0.7427719235420227, 'gen_loss: ', 0.17870275676250458)\n",
      "('recon_loss:', 0.06325462460517883, 'disc_loss:', 0.7257862091064453, 'gen_loss: ', 0.1936703771352768)\n",
      "('recon_loss:', 0.06476203352212906, 'disc_loss:', 0.7294236421585083, 'gen_loss: ', 0.18967042863368988)\n",
      "('recon_loss:', 0.06615598499774933, 'disc_loss:', 0.7261171936988831, 'gen_loss: ', 0.19180573523044586)\n",
      "('recon_loss:', 0.06504283845424652, 'disc_loss:', 0.7217482328414917, 'gen_loss: ', 0.1977507472038269)\n",
      "('recon_loss:', 0.04593069851398468, 'disc_loss:', 0.7291936278343201, 'gen_loss: ', 0.18814855813980103)\n",
      "('recon_loss:', 0.05143599212169647, 'disc_loss:', 0.7284528017044067, 'gen_loss: ', 0.18929573893547058)\n",
      "('recon_loss:', 0.05079161748290062, 'disc_loss:', 0.7347224950790405, 'gen_loss: ', 0.18168644607067108)\n",
      "('recon_loss:', 0.05135192722082138, 'disc_loss:', 0.7296393513679504, 'gen_loss: ', 0.1869553029537201)\n",
      "('recon_loss:', 0.04783956706523895, 'disc_loss:', 0.7332226634025574, 'gen_loss: ', 0.18115516006946564)\n",
      "('recon_loss:', 0.05176859721541405, 'disc_loss:', 0.7274290919303894, 'gen_loss: ', 0.18977956473827362)\n",
      "('recon_loss:', 0.04214297607541084, 'disc_loss:', 0.7399634122848511, 'gen_loss: ', 0.17499497532844543)\n",
      "('recon_loss:', 0.039578549563884735, 'disc_loss:', 0.7275127172470093, 'gen_loss: ', 0.19002847373485565)\n",
      "('recon_loss:', 0.03712135925889015, 'disc_loss:', 0.735958993434906, 'gen_loss: ', 0.17991599440574646)\n",
      "('recon_loss:', 0.04737885668873787, 'disc_loss:', 0.72566157579422, 'gen_loss: ', 0.19237352907657623)\n"
     ]
    }
   ],
   "source": [
    "Q,P = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entire_batch,batch_labels = get_train_batch(no_examples)\n",
    "z_encoded = Q(entire_batch)\n",
    "batch_labels_np = batch_labels.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x_encoded = z_encoded.cpu().data.numpy()[:,0]\\ny_encoded = z_encoded.cpu().data.numpy()[:,1]\\nw_encoded = z_encoded.cpu().data.numpy()[:,2]\\n\\nbatch_labels_np = batch_labels_np.astype(int)\\nprint(batch_labels_np.dtype)\\nprint(batch_labels_np.shape)\\nbatch_labels_np = list(batch_labels_np.reshape(no_examples,1))\\n\\ncolors = []\\nfor l in batch_labels_np:\\n    colors.append(\"C\"+str(int(l)))\\n    \\n#plt.scatter(x_encoded,y_encoded,c=colors)\\nfig = plt.figure()\\nax = Axes3D(fig)\\nax.scatter(x_encoded,y_encoded,w_encoded,c=colors)\\nplt.show()'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''x_encoded = z_encoded.cpu().data.numpy()[:,0]\n",
    "y_encoded = z_encoded.cpu().data.numpy()[:,1]\n",
    "w_encoded = z_encoded.cpu().data.numpy()[:,2]\n",
    "\n",
    "batch_labels_np = batch_labels_np.astype(int)\n",
    "print(batch_labels_np.dtype)\n",
    "print(batch_labels_np.shape)\n",
    "batch_labels_np = list(batch_labels_np.reshape(no_examples,1))\n",
    "\n",
    "colors = []\n",
    "for l in batch_labels_np:\n",
    "    colors.append(\"C\"+str(int(l)))\n",
    "    \n",
    "#plt.scatter(x_encoded,y_encoded,c=colors)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(x_encoded,y_encoded,w_encoded,c=colors)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(size):\n",
    "    ind = torch.cuda.LongTensor(torch.randperm(no_examples)[:size].numpy())\n",
    "    return z_encoded[ind], batch_labels[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_disc(d,d_optim):\n",
    "    for ep in range(2000):\n",
    "        d_optim.zero_grad()\n",
    "        x,true_l = sample_z(50)\n",
    "        true_l = true_l.view(true_l.size()[0],)\n",
    "        p_labels = d(x)\n",
    "        weights = torch.Tensor([1,10]).cuda()\n",
    "        criteria = nn.CrossEntropyLoss(weight=weights)\n",
    "        true_l = true_l.type(torch.cuda.LongTensor)\n",
    "        loss = criteria(p_labels,true_l)\n",
    "        loss.backward(retain_variables = True)\n",
    "        d_optim.step()\n",
    "        \n",
    "        if(ep%50==49):\n",
    "            print(loss.data[0])\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator,self).__init__()\n",
    "        self.l1 = nn.Linear(z_dim,400)\n",
    "        self.l2 = nn.Linear(400,400)\n",
    "        self.l3 = nn.Linear(400,70)\n",
    "        self.l4 = nn.Linear(70,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.tanh(self.l1(x))\n",
    "        x = F.tanh(self.l2(x))\n",
    "        x = F.tanh(self.l3(x))\n",
    "        x = (self.l4(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_disc_model():\n",
    "    d = Discriminator()\n",
    "    d.cuda()\n",
    "    d_optim = optim.Adam(d.parameters(),lr=1e-4)\n",
    "    d = train_disc(d,d_optim)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.484246611595\n",
      "0.197930753231\n",
      "0.206379532814\n",
      "0.13697257638\n",
      "0.102194577456\n",
      "0.535872042179\n",
      "0.174371153116\n",
      "0.27784961462\n",
      "0.283189356327\n",
      "0.260330915451\n",
      "0.204557299614\n",
      "0.135556429625\n",
      "0.0916865393519\n",
      "0.0415781922638\n",
      "0.130641147494\n",
      "0.176254764199\n",
      "0.149524062872\n",
      "0.218980535865\n",
      "0.111575298011\n",
      "0.037403807044\n",
      "0.0844438448548\n",
      "0.121269144118\n",
      "0.0809474438429\n",
      "0.0588055811822\n",
      "0.0393026173115\n",
      "0.043043538928\n",
      "0.0594484135509\n",
      "0.0616584680974\n",
      "0.133588597178\n",
      "0.0627502799034\n",
      "0.0601399801672\n",
      "0.175496920943\n",
      "0.186965689063\n",
      "0.488889932632\n",
      "0.0300031118095\n",
      "0.0738388374448\n",
      "0.107866667211\n",
      "0.0545143298805\n",
      "0.0103065669537\n",
      "0.0345447957516\n"
     ]
    }
   ],
   "source": [
    "d = gen_disc_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#entire_batch,batch_labels = get_train_batch(no_examples)\n",
    "train_encoded = Q(Variable(torch.cuda.FloatTensor(p_fingerprints)))\n",
    "train_op = d(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train_op.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tn, fp, fn, tp: ', array([3324,   52,   16,   31]))\n"
     ]
    }
   ],
   "source": [
    "train_op = train_op.cpu().data.numpy()\n",
    "train_op = np.argmax(train_op,axis=1)\n",
    "#batch_labels = batch_labels.view(batch_labels.size()[0],1)\n",
    "#batch_labels = batch_labels.data.cpu().numpy()\n",
    "#print(batch_labels)\n",
    "#batch_labels = batch_labels.astype(int)\n",
    "cf = metrics.confusion_matrix(l,train_op)\n",
    "print('tn, fp, fn, tp: ',cf.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 47.]\n"
     ]
    }
   ],
   "source": [
    "print(sum(batch_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
